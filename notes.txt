########################################
########## HELPFUL COMMANDS
########################################

show dbs                     	show database names
show collections             	show collections in current database
show users                   	show users in current database
db.getName();					Get current database name
use <db_name>					Set current database
db.dropDatabase();				Delete/drop current database
show collections				Show the collections contained in the current database

mvn clean compile exec:java -Dexec.mainClass=com.mongdb.App

> docker run -it -p 27017:27017 --name myMongo -d mongo
> docker exec -it myMongo mongo

########################################
########## WEEK 2 - CRUD
#### TOPICS: Mongo Shell, Query Operators, Update Operators and a Few Commands

##### Creating documents

mongorestore dump
insertOne();
insertMany();
UPDATE COMMANDS is known as "upserts".

##### The _id Field

All collections have a unique primary index on the _id field by default.

			Date    MAC		PID		Counter
ObjectId: _ _ _ _ | _ _ _ | _ _ | _ _ _
12-Byte Hex String

Date - Current time stamp
MAC - The MAC address for the machine on which the MongoDB server is running.
PID - Process ID.
Counter - A standard counter to make sure that the ObjectId is indeed unique.

Note that the "Date" label that refers to the first four bytes does in fact reference a UTC datetime with a precision of seconds (not days). When talking about BSON, this is distinct from a timestamp, which is a special internal data type used in MongoDB replication and sharding.

##### Reading documents

db.movieDetails.find({ rated: "PG-13" }).pretty();
db.movieDetails.find({ rated: "PG-13" }).count();
db.movieDetails.find({ rated: "PG-13", year: 2009 }).count();
db.movieDetails.find({ "tomato.meter": 100 }).count();
db.movieDetails.find({ "tomato.meter": 99 }).pretty();

// For this particular case the order of the elements matters
// So the writers array must have only two elements following the order which each one was inserted
db.movieDetails.find({ "writers": ["Ethan Coen", "Joel Coen"] }).count();

// In this very case it will match all documents which have the actor "Jeff Bridges", thus this is not an exact match like the previous one
db.movieDetails.find({ "actors": "Jeff Bridges" }).pretty();

// All documents which have "Jeff Bridges" as the first element in actors array
db.movieDetails.find({ "actors.0": "Jeff Bridges" }).pretty();

// The find method returns a cursor. If you do the command below without assigning a variable to it, the cursor will automatically iterate up to 20 times.
db.movieDetails.find({ rated: "PG" }).pretty();

// Cursors

var c = db.movieDetails.find();
var doc = function() { return c.hasNext() ? c.next() : null; }
c.objsLeftInBatch(); // 101 documents in the initial batch
doc() // to iterate using the cursor
c.objsLeftInBatch(); // if will do the command again it will return 100 documents

// Projectinos

// It will include only the attribute title and exclude _id
db.movieDetails.find({ rated: "PG" }, { title: 1, _id: 0 } ).pretty();

// It will include all fields except for the ones explicity excluded below
db.movieDetails.find( { rated: "PG" }, { writers: 0, actors: 0, _id: 0 } ).pretty();

##### Comparison operators

// More details at https://docs.mongodb.com/manual/reference/operator/query-comparison/

// All movies that have a runtime greater than 90
db.movieDetails.find({ runtime: { $gt: 90 } }).pretty();
db.movieDetails.find({ runtime: { $gt: 90 } }).count();
db.movieDetails.find({ runtime: { $gt: 90 } }, { title: 1, runtime: 1, _id: 0 }).pretty();
// Greater than or equal to 90 and less than or equal to 120. It means: (runtime >= 90 && runtime <= 120)
db.movieDetails.find({ runtime: { $gte: 90, $lte: 120 } }, { title: 1, runtime: 1, _id: 0 }).pretty();
db.movieDetails.find({ "tomato.meter": { $gte: 95 }, runtime: { $gt: 180 } }, { title: 1, runtime: 1, _id: 0 }).pretty();
db.movieDetails.find({ rated: { $ne: "UNRATED" } }, { title: 1, runtime: 1, _id: 0 }).count();
// Return all the documents where the value of rated is G or PG
db.movieDetails.find({ rated: { $in: ["G", "PG"] } }, { title: 1, runtime: 1, _id: 0 }).count();
// The reverse statement of the prepostion above
db.movieDetails.find({ rated: { $nin: ["G", "PG"] } }, { title: 1, runtime: 1, _id: 0 }).count();

##### Element operators

// It verifies if there are documents which have tomate.meter field
db.movieDetails.find({ "tomato.meter": { $exists: true } }).count();

db.moviesScratch.find({ _id: { $type: "string" } }).pretty();
// We can count how many documents are stored with _id as String and so on
db.moviesScratch.find({ _id: { $type: "string" } }).count();
db.moviesScratch.find({ _id: { $type: "objectId" } }).pretty();

##### Logical operators

// More details at https://docs.mongodb.com/manual/reference/operator/query-logical/

// $or takes an array as an argument
db.movieDetails.find({ $or : [ { "tomato.meter": { $gt: 99 } },
                               { "metacritic": { $gt: 95 } } ] }).count();

// $and is used when we need to specify multiple criteria on the same field							   
db.movieDetails.find({ $and : [ { "metacritic": { $ne: null } },
                               { "metacritic": { $exists: true } } ] }).count();
							   
##### Regex operator

// Return all the documents which have awards.text field started by the word "Won ". "\s" means "space".
db.movieDetails.find({ "awards.text": { $regex: /^Won\s.*/ } }).pretty()

##### Array operators

// More details at https://docs.mongodb.com/manual/reference/operator/query-array/

// Return all documents which have comedy, crime and drame in their genres field.
db.movieDetails.find({ genres: { $all: ["Comedy", "Crime", "Drama"] } }).pretty()	

// This is entire different from the example using $elemMatch because it won't try by each element contained in the array.
// Actually each field will be evaluated individually
db.movieDetails.find({ boxOffice: { country: "UK", revenue: { $gt: 15 } } })

db.movieDetails.find({ boxOffice: {$elemMatch: { country: "UK", revenue: { $gt: 15 } } } })

##### Updating documents

// Update any document that has imbd.id equal to detail.imdb.id
// Replace the document with detail
// If this filter doesn't match any document in my collection, so I want to go ahead and create a new one, with a new _id
db.movieDetails.updateOne(
	{ "imbd.id" : detail.imdb.id },
	{ $set : detail },
	{ upsert: true }
)

##### Homework 2.1

mvn compile exec:java -Dexec.mainClass=course.homework.MongoDBSparkFreemarkerStyle

##### Homework 2.2

mongoimport -d students -c grades < grades.json
use students
db.grades.count() // You should get 800.
// The student_id with the highest average score
db.grades.aggregate({'$group':{'_id':'$student_id', 'average':{$avg:'$score'}}}, {'$sort':{'average':-1}}, {'$limit':1})
// All exam scores greater than or equal to 65, and sort those scores from lowest to highest.
db.grades.find({ score: { $gte: 65 } }).sort({ 'score': 1 });

##### Homework 2.3

// Sort student by its identity and limit the query to 12
db.grades.find().sort({'student_id' : 1}).limit(12);

// Remove the grade of type "homework" with the lowest score for each student from the dataset in the handout. 
// Since each document is one grade, it should remove one document per student. Sample:
/*
> db.grades.find({"student_id" : 0})
{ "_id" : ObjectId("50906d7fa3c412bb040eb577"), "student_id" : 0, "type" : "exam", "score" : 54.6535436362647 }
{ "_id" : ObjectId("50906d7fa3c412bb040eb579"), "student_id" : 0, "type" : "homework", "score" : 14.8504576811645 }
{ "_id" : ObjectId("50906d7fa3c412bb040eb578"), "student_id" : 0, "type" : "quiz", "score" : 31.95004496742112 }
{ "_id" : ObjectId("50906d7fa3c412bb040eb57a"), "student_id" : 0, "type" : "homework", "score" : 63.98402553675503 }
*/

db.grades.aggregate(
	{
		"$match" : { "type" : "homework" }
	},
	{
		'$group': {
		'_id' : { student_id : "$student_id", type : "$type" },
		'min' : { '$min' : '$score' }
		}
	}, 
	{'$sort':{'_id':-1}}, 
	{'$limit':1});

// The result should be 600. Now let us find the student who holds the 101st best grade across all grades:
db.grades.find().sort( { 'score' : -1 } ).skip( 100 ).limit( 1 )	
// Now let us sort the students by student_id , and score, while also displaying the type to then see what the top five docs are:
db.grades.find( { }, { 'student_id' : 1, 'type' : 1, 'score' : 1, '_id' : 0 } ).sort( { 'student_id' : 1, 'score' : 1 } ).limit( 5 )
// The identity of the student with the highest average in the class
db.grades.aggregate( { '$group' : { '_id' : '$student_id', 'average' : { $avg : '$score' } } }, { '$sort' : { 'average' : -1 } }, { '$limit' : 1 } )
	
##### Homework 2.5

// Which of the choices below is the title of a movie from the year 2013 that is rated PG-13 and won no awards?

db.movieDetails.find({ "rated": "PG-13", "year": 2013, "awards.wins": 0})	

##### Homework 2.6

// Using the video.movieDetails collection, how many movies list "Sweden" second in the the list of countries.

db.movieDetails.find({ "countries.1": "Sweden" })	

########################################
########## WEEK 3 - SCHEMA DESIGN
#### TOPICS: Patterns, Case Studies & Tradeoffs

// --------- Introduction

// https://en.wikipedia.org/wiki/Third_normal_form

// Application driven schema

MongoDB keys: 
	1 - Rich documents
	2 - Pre Join / Embed Data
	3 - No Mongo Joins - it doesn't support joins directly inside de kernel, you must do it inside de application
	4 - There are no constraints
	5 - Atomic operations - MongoDB doesn't support transactions, but it does support atomic operations within one document.
	6 - No declared schema

What's the single most important factor in designing your application schema within MongoDB?
A: Matching the data access patterns of your application

What's the single most important factor in designing your application schema within MongoDB?
A: Matching the data access patterns of your application

// --------- Living wihout transactions

Restructure to create a single document and then make good use of atomic operations
Implement transaction in software like MongoDB critical section
And at last tolerate a little bit of inconsistency

Which of the following operations operate atomically within a single document? Check all that apply.
Update, findAndModify, $addToSet(within an update) and $push within an update

// --------- One to one relations

Frequency of access
Size of the items
Atomicity of data

// --------- One to many relations	

One modest type of one to many is when you think about people vs city situation. It's complicated to make a good relationship with MongoDB, true linking is one way to resolve it.

// --------- One to few relations	

It's like one to many, but a lot easier with MongoDB. Blog post vs comments is a typical  situation.

// --------- Many to many relations

Samples situations are books vs authors and students vs teachers. As you can see they are not truly many to many but few to few, so you can linking or embedding, of course you must evaluate pretty well which one best suits you.

// --------- Multikeys indexes

> db.students.find()
{ "_id" : 0, "name" : "Andrew Erlichson", "teachers" : [ 0, 1 ] }
{ "_id" : 1, "name" : "Richard Kreuter", "teachers" : [ 0, 1, 3 ] }
{ "_id" : 2, "name" : "Eliot Horowitz", "teachers" : [ 1, 2, 3 ] }
{ "_id" : 3, "name" : "Mark Heinrich", "teachers" : [ 0, 3 ] }
> db.teachers.find()
{ "_id" : 0, "name" : "Mark Horowitz" }
{ "_id" : 1, "name" : "John Hennessy" }
{ "_id" : 2, "name" : "Bruce Wolley" }
{ "_id" : 3, "name" : "James Plummer" }
> db.students.ensureIndex({ 'teachers' : 1 })
// All the students who had Mark Horowitz and John Hennessy as professors
> db.students.find({ 'teachers' : { $all : [0,1] } )
{ "_id" : 0, "name" : "Andrew Erlichson", "teachers" : [ 0, 1 ] }
{ "_id" : 1, "name" : "Richard Kreuter", "teachers" : [ 0, 1, 3 ] }
// How do we know that used an index?
> db.students.find({ 'teachers' : { $all : [0,1] } ).explain()

// --------- Benefits of embedding

The main benefit of embedding data from two different collections and bringing it together into one collection is performance.

- Improved read performance
- One round trip to the DB

// --------- Representing trees

https://docs.mongodb.com/manual/tutorial/model-tree-structures-with-ancestors-array/

Given the following typical document for a e-commerce category hierarchy collection called categories.

{
  _id: 34,
  name: "Snorkeling",
  parent_id: 12,
  ancestors: [12, 35, 90]
}

Which query will find all descendants of the snorkeling category?

db.categories.find({ ancestors : 34 })

// --------- ODM introduction

https://mongodb.github.io/morphia/

ODM means Object Document Mapper.

##### Homework 3.1

// Write a program in the language of your choice that will remove the lowest homework score for each student. Since there is a single document for each student containing an array of scores, you will need to update the scores array and remove the homework.

// Remember, just remove a homework score. Don't remove a quiz or an exam!

// Hint/spoiler: With the new schema, this problem is a lot harder and that is sort of the point. One way is to find the lowest homework in code and then update the scores array with the low homework pruned.

use school
db.students.count() // should return 200
db.students.find( { _id : 137 } ).pretty()

{
	"_id" : 137,
	"name" : "Tamika Schildgen",
	"scores" : [
		{
			"type" : "exam",
			"score" : 4.433956226109692
		},
		{
			"type" : "quiz",
			"score" : 65.50313785402548
		},
		{
			"type" : "homework",
			"score" : 89.5950384993947
		},
		{
			"type" : "homework",
			"score" : 54.75994689226145
		}
	]
}

db.students.find( { _id : 19 } ).pretty()

{
	"_id" : 19,
	"name" : "Gisela Levin",
	"scores" : [
		{
			"type" : "exam",
			"score" : 44.51211101958831
		},
		{
			"type" : "quiz",
			"score" : 0.6578497966368002
		},
		{
			"type" : "homework",
			"score" : 93.36341655949683
		},
		{
			"type" : "homework",
			"score" : 49.43132782777443
		}
	]
}

// Annotations

> db.students.aggregate([ {'$sort':{'_id':-1}},  {'$limit':4} ]);
{ "_id" : 199, "name" : "Rae Kohout", "scores" : [ { "type" : "exam", "score" : 82.11742562118049 }, { "type" : "quiz", "score" : 49.61
295450928224 }, { "type" : "homework", "score" : 28.86823689842918 }, { "type" : "homework", "score" : 5.861613903793295 } ] }
{ "_id" : 198, "name" : "Timothy Harrod", "scores" : [ { "type" : "exam", "score" : 11.9075674046519 }, { "type" : "quiz", "score" : 20
.51879961777022 }, { "type" : "homework", "score" : 55.85952928204192 }, { "type" : "homework", "score" : 64.85650354990375 } ] }
{ "_id" : 197, "name" : "Tonisha Games", "scores" : [ { "type" : "exam", "score" : 38.51269589995049 }, { "type" : "quiz", "score" : 31
.16287577231703 }, { "type" : "homework", "score" : 79.15856355963004 }, { "type" : "homework", "score" : 56.17504143517339 } ] }
{ "_id" : 196, "name" : "Santiago Dollins", "scores" : [ { "type" : "exam", "score" : 52.04052571137036 }, { "type" : "quiz", "score" :
 33.63300076481705 }, { "type" : "homework", "score" : 4.629511012591447 }, { "type" : "homework", "score" : 78.79257377604428 } ] }
 
// https://docs.mongodb.com/manual/reference/operator/aggregation/unwind/ 
 
> db.students.aggregate([ { $unwind: "$scores" }, {'$sort':{'_id':-1}},  {'$limit':4} ]);
{ "_id" : 199, "name" : "Rae Kohout", "scores" : { "type" : "homework", "score" : 5.861613903793295 } }
{ "_id" : 199, "name" : "Rae Kohout", "scores" : { "type" : "quiz", "score" : 49.61295450928224 } }
{ "_id" : 199, "name" : "Rae Kohout", "scores" : { "type" : "homework", "score" : 28.86823689842918 } }
{ "_id" : 199, "name" : "Rae Kohout", "scores" : { "type" : "exam", "score" : 82.11742562118049 } }

> db.students.aggregate([ { $unwind: "$scores" }, { $group: { '_id' : { _id : "$_id", type : "$scores.type" }, 'min' : { '$min' : '$scores.score' } } }, {'$sort':{'_id':-1}},  {'$limit':3} ]);
{ "_id" : { "_id" : 199, "type" : "quiz" }, "min" : 49.61295450928224 }
{ "_id" : { "_id" : 199, "type" : "homework" }, "min" : 5.861613903793295 }
{ "_id" : { "_id" : 199, "type" : "exam" }, "min" : 82.11742562118049 }

> db.students.aggregate([ { $unwind: "$scores" }, { $group: { '_id' : { _id : "$_id", type : "$scores.type" }, 'min' : { '$min' : '$scores.score' } } }, {'$sort':{'_id':-1}},  {'$limit':4} ]);
{ "_id" : { "_id" : 199, "type" : "quiz" }, "min" : 49.61295450928224 }
{ "_id" : { "_id" : 199, "type" : "homework" }, "min" : 5.861613903793295 }
{ "_id" : { "_id" : 199, "type" : "exam" }, "min" : 82.11742562118049 }
{ "_id" : { "_id" : 198, "type" : "quiz" }, "min" : 20.51879961777022 }

> db.students.aggregate([ { $unwind: "$scores" }, { $match : { "scores.type" : "homework" } }, { $group: { '_id' : { _id : "$_id", type: "$scores.type" }, 'min' : { '$min' : '$scores.score' } } }, {'$sort':{'_id':-1}},  {'$limit':4} ]);
{ "_id" : { "_id" : 199, "type" : "homework" }, "min" : 5.861613903793295 }
{ "_id" : { "_id" : 198, "type" : "homework" }, "min" : 55.85952928204192 }
{ "_id" : { "_id" : 197, "type" : "homework" }, "min" : 56.17504143517339 }
{ "_id" : { "_id" : 196, "type" : "homework" }, "min" : 4.629511012591447 }

db.students.aggregate([	
	{ $unwind: "$scores" },
	{ $match : { "scores.type" : "homework" } },
	{ $group: {
			'_id' : { _id : "$_id", type : "$scores.type" },
			'min' : { '$min' : '$scores.score' }
		}
	},	
	{'$sort':{'_id':-1}}, 
	{'$limit':1}
]);

https://docs.mongodb.com/manual/reference/operator/update/positional/#update-embedded-documents-using-multiple-field-matches
https://docs.mongodb.com/manual/reference/operator/update/pull/#remove-items-from-an-array-of-documents

db.students.find({ "_id": 199 })

db.students.update(
	{ "_id": 199 },
	{ $pull: { scores: { type: "homework" , score: 5.861613903793295 } } },
	{ multi: true }
)

// To verify that you have completed this task correctly, provide the identity (in the form of their _id) of the student with the highest average in the class with following query that uses the aggregation framework. The answer will appear in the _id field of the resulting document.

db.students.aggregate( [
  { '$unwind': '$scores' },
  {
    '$group':
    {
      '_id': '$_id',
      'average': { $avg: '$scores.score' }
    }
  },
  { '$sort': { 'average' : -1 } },
  { '$limit': 1 } ] )
  
########################################
########## WEEK 4 - PERFORMANCE  
#### TOPICS: Using Indexes, Monitoring And Understanding Performance. Performance In Sharded Environments

// --------- Storage Engines: Introduction

https://docs.mongodb.com/manual/core/storage-engines/

Main ways of performance:
	- Indexing
	- Sharding - Distributing database queries across multiple servers
	
Storage engine: It is located between MongoDB itself and for example a type of persistence storage like hard disk. It can use memory to storage data before send it to disk. If we compare to something real it's like a motor engine in your car, you can switch it for another when you like it to match some sort of requirement. MongoDB ships with two storage engines:
	- MMAP: It's the default for 3.0.
	- WiredTiger: It started to be the default on 3.2 --> https://docs.mongodb.com/manual/release-notes/3.2/#wiredtiger-as-default

The storage engine directly determines which of the following?
A: The data file format; Format of indexes.

// --------- Storage Engines: MMAPv1

1 - Collection level locking
2 - In place updates

http://docs.mongodb.org/manual/core/storage/?&_ga=2.176324003.721559069.1498430831-1068873678.1493258930#power-of-2-allocation

> man mmap // The description is "allocate memory, or map files or devices into memory"

Which of the following statements about the MMAPv1 storage engine are true? 
A: MMAPv1 automatically allocates power-of-two-sized documents when new documents are inserted; MMAPv1 is built on top of the mmap system call that maps files into memory.

Example that has record spaces allocated for as few as 4 bytes; in fact, the minimum record space in MongoDB 3.0 is 32 bytes.

// --------- Storage Engines: WiredTiger

1 - Document level concurrency
2 - Compressions of DATA and INDEXES
3 - No in place update

You can use this storage engine using -storageEngine. Sample:
> mongod -dbpath myCustomDir -storageEngine wiredTiger
> db.foo.insert({ name: 'Budega' })
> db.foo.stats(); // See the properties of the collection

Which of the following are features of the WiredTiger storage engine?
A: Document-level concurrency; Compression.

// --------- Indexes

https://docs.mongodb.com/manual/indexes/
https://en.wikipedia.org/wiki/B-tree -> A type of balanced search tree used frequently for indexing database tables

(a, b, c)
	a - OK
	ab - OK
	abc - OK
	b - NO
	c - NO
	And so on...
	
Writes: Indexes actually slows down your writes... Writes are much faster when there is no index
Read: But READS will be much faster!

Which optimization will typically have the greatest impact on the performance of a database?
A: Adding appropriate indexes on large collections so that only a small percentage of queries need to scan the collection.

// --------- Creating indexes

Use create_scores2.6f9f5645026a.js file to create 1.000.000 documents.
> db.students.find({ student_id: 5 }) // It will take a bunch of time
> db.students.explain().find({ student_id: 5 })
// On winningPlan you can see which type of scan was used during the query. For our case is COLLSCAN, it means that the collection was fully scanned (all the documents) in order to retrieve what was asked.
> db.students.findOne({ student_id: 5 }) // Faster than the find() option
> db.students.createIndex({ student_id: 1 }) // We want to create index on student_id ascending
> db.students.explain().find({ student_id: 5 })
> db.students.explain(true).find({ student_id: 5 }) // You can see how much documents were used in order to retrieve what was asked
> db.students.createIndex({ student_id: 1, class_id: -1 }) // student_id ascending and class_id descending

// --------- Discovering and Deleting Indexes

> db.students.getIndexes(); // There is default index which is on _id and you can't delete it
> db.students.dropIndexex({ student_id: 1 })

// --------- Multikey Indexes

Creating indexes on arrays is known as "multikey indexes".

{
	name : 'Budega',
	tags : [ 'tennis', 'programming', 'music' ],
	color : 'blue',
	location : ['SP', 'RJ']
}

Legal indexes: (tags), (tags, color)
Illegal indexes: (tags, location) -> Multikey indexes doesn't allow compound index to have two arrays

> db.foo.insert({ a : 1, b: 2 })
> db.foo.find()
> db.foo.createIndex({ a : 1, b : 1 })
> db.foo.explain().find({ a : 1, b : 1 }) // See indexName and isMultiKey fields
> db.foo.insert({ a : 3, b: [3,5,7] })
> db.foo.explain().find({ a : 1, b : 1 }) // See isMultiKey equals true now
> db.foo.explain().find({ a : 1, b : 5 })
> db.foo.getIndexes();
> db.foo.insert({ a : [3,4,6], b: [7,8,9] }) // It won't work: cannot index parallel arrays [b] [a]
> db.foo.insert({ a : [3,4,6], b: 8 }) // It works because it doesn't have parallel arrays

// --------- Dot notation and multikey

https://docs.mongodb.com/manual/reference/operator/query/elemMatch/

> db.students.createIndex({ 'scores.score' : 1 })
> db.students.explain().find({ 'scores.score' : { '$gt' : 99 } })
> db.students.find({ 'scores' : { '$elemMatch' : { type : 'exam', score : { '$gt' : 99.8 } } } }).count()
> db.students.explain().find({ 'scores' : { '$elemMatch' : { type : 'exam', score : { '$gt' : 99.8 } } } }) // You can see that the index search is made and then elemMatch ran in each document which were retrieved
> db.students.explain(true).find({ 'scores' : { '$elemMatch' : { type : 'exam', score : { '$gt' : 99.8 } } } }) // Just to make sure that the above statement is true... See docsExamined field.
> db.students.find({ 'scores.score' : { '$gt' : 99.8 } }).count() // It must matched docsExamined above
> db.students.find({ '$and' : [{ 'scores.type' : 'exam' }, { 'scores.score' : { '$gt' : 99.8 } }] }) // It does not work as elemMatch as you may get documents which you didn't want to
> db.students.explain().find({ '$and' : [{ 'scores.type' : 'exam' }, { 'scores.score' : { '$gt' : 99.8 } }] })

Suppose you have a collection called people in the database earth with documents of the following form:

{
    "_id" : ObjectId("551458821b87e1799edbebc4"),
    "name" : "Eliot Horowitz",
    "work_history" : [
        {
            "company" : "DoubleClick",
            "position" : "Software Engineer"
        },
        {
            "company" : "ShopWiki",
            "position" : "Founder & CTO"
        },
        {
            "company" : "MongoDB",
            "position" : "Founder & CTO"
        }
    ]
}

Type the command that you would issue in the Mongo shell to create an index on company, descending.

> db.people.createIndex({ 'work_history.company' : -1 })


// --------- Index Creation Options, Unique

https://docs.mongodb.com/v3.0/reference/method/db.collection.createIndex/#options

> db.stuff.drop();
> db.stuff.insert({ 'thing' : 'apple' })
> db.stuff.insert({ 'thing' : 'pear' })
> db.stuff.insert({ 'thing' : 'apple' })
> db.stuff.find()
> db.stuff.createIndex({ 'thing' : 1 })
> db.stuff.dropIndex({ 'thing' : 1 }, { unique: true }) // Error because there are two apples
> db.stuff.remove({ 'thing' : 'apple' }, { justOne: true })
> db.stuff.dropIndex({ 'thing' : 1 }, { unique: true }) // It works
> db.stuff.insert({ 'thing' : 'pear' }) // Duplicate key error
> db.stuff.getIndexes()

Please provide the mongo shell command to create a unique index on student_id, class_id, ascending for the collection students.
> db.students.createIndex({ student_id: 1, class_id: 1 }, { unique : true })

// --------- Sparse Indexes

https://docs.mongodb.com/manual/indexes/#sparse-indexes

It's a sort of index that can be used when the index key is missing from some of documents.
When you specify this options you tells MongoDB that it should not include in the index documents that are missing the key.

> db.employees.createIndex({ cell: 1}, { unique : true, sparse, true } })

One advantage of using sparse index is that it uses a lot less space than the common one.

What are the advantages of a sparse index?
A: The index will be smaller than it would if it were not sparse; You can gain greater flexibility with creating Unique indexes.

// --------- Index creation, background

http://docs.mongodb.org/manual/core/index-creation/?_ga=2.144413107.721559069.1498430831-1068873678.1493258930#index-creation-background

• Foreground
	1 - Fast
	2 - Blocks writers and readers in the database, it means... DO NOT DO IT IN PRODUCTION SYSTEMS!
• Background
	1 - Slower
	2 - DON'T block writers and readers in the database, it means... DO NOT DO IT IN PRODUCTION SYSTEMS!
	
> db.students.createIndex({ 'score.score' : 1 }, { background : true })

Which things are true about creating an index in the background in MongoDB.
A: Although the database server will continue to take requests, a background index creation still blocks the mongo shell that you are using to create the index; Creating an index in the background takes longer than creating it in the foreground.

Note that, in MongoDB 2.2, the answer that you can only make one background index per database would have been correct.

// --------- Using explain

https://docs.mongodb.com/manual/reference/explain-results/

Explain is used to find out what the database is doing with your query, how it's executing it, what indexes it's using and how many documents it inspected when it actually runs the query. It didn't bring data to the client, only the "explain" information.

> var exp = db.foo.explain()
> exp.help()

// --------- Explain: verbosity

Explain runs using "query planner" mode by default and we're using it like that so far. There are two other modes:
	0 - Query planner: Standard one. It tells you mostly what the database would use in terms of the indexes
	1 - Execution stats: It includes query planner and it tells the results of the query like number of documents returned, how much time it will take to accomplish the query and so on.
	2 - All plans execution: It includes query planner and execution stats. When you run using this option actually you're running the query optimizer that periodically runs to determine what index would be used for any particular shape of query.
> var exp = db.example.explain("executionStats")
> exp.find({ a : 17, b : 55 })
> db.example.dropIndex({ a : 1, b : 1 })
> exp.find({ a : 17, b : 55 })
> db.example.createIndex({ a : 1, b : 1 })
> var exp = db.example.explain("allPlansExecution")
> exp.find({ a : 17, b : 55 })

Given the following output from explain, what is the best description of what happened during the query?

> exp = db.example.explain("executionStats")
Explainable(test.example)
> exp.find( { a : 7 } )
{
    "queryPlanner" : {
        "plannerVersion" : 1,
        "namespace" : "test.example",
        "indexFilterSet" : false,
        "parsedQuery" : {
            "a" : {
                "$eq" : 7
            }
        },
        "winningPlan" : {
            "stage" : "COLLSCAN",
            "filter" : {
                "a" : {
                    "$eq" : 7
                }
            },
            "direction" : "forward"
        },
        "rejectedPlans" : [ ]
    },
    "executionStats" : {
        "executionSuccess" : true,
        "nReturned" : 10000,
        "executionTimeMillis" : 619,
        "totalKeysExamined" : 0,
        "totalDocsExamined" : 999999,
        "executionStages" : {
            "stage" : "COLLSCAN",
            "filter" : {
                "a" : {
                    "$eq" : 7
                }
            },
            "nReturned" : 10000,
            "executionTimeMillisEstimate" : 520,
            "works" : 1000001,
            "advanced" : 10000,
            "needTime" : 990000,
            "needFetch" : 0,
            "saveState" : 7812,
            "restoreState" : 7812,
            "isEOF" : 1,
            "invalidates" : 0,
            "direction" : "forward",
            "docsExamined" : 999999
        }
    },
    "serverInfo" : {
        "host" : "cross-mb-air.local",
        "port" : 27017,
        "version" : "3.0.1",
        "gitVersion" : "534b5a3f9d10f00cd27737fbcd951032248b5952"
    },
    "ok" : 1
}

A: The query scanned 999,999 documents, returning 10,000 in 619 milliseconds.

// --------- Covered Queries

https://www.tutorialspoint.com/mongodb/mongodb_covered_queries.htm
https://docs.mongodb.com/manual/core/query-optimization/#covered-query

A covered query is a query that can be satisfied entirely using an index and does not have to examine any documents.	

You would like to perform a covered query on the example collection. You have the following indexes:

{ name : 1, dob : 1 }
{ _id : 1 }
{ hair : 1, name : 1 }

Which of the following is likely to be a covered query?
A: db.example.find( { name : { $in : [ "Bart", "Homer" ] } }, {_id : 0, dob : 1, name : 1} )

// --------- When is an index used?

We no longer evict plans from the cache after a threshold number of writes. Instead, we evict when the "works" of the first portion of the query exceed the number of "works" used to decide on the winning plan by a factor of 10x. You can see the "works" for a particular plan using .explain("executionStats").

Let's suppose we have the following indexes:
	1 - b,c
	2 - c,d
	3 - d,e
	4 - e,f
	5 - a,b,c
When we issue a query where it may use index 1, 2 or 5, MongoDB will then create three query plans. In three parallel threads issue the query such that each one will use a different index and see which one is able to return results the fastest.

The WINNING QUERY PLAN in stored in a CACHE for future use for queries of that shape. The cache is erased using one of the following:
1 - Threshold (number of writes)
2 - Rebuild the index
3 - Any index is added or dropped from the collection
4 - mongod proccess is restarted

Given collection foo with the following index:

db.foo.createIndex( { a : 1, b : 1, c : 1 } )

Which of the following queries will use the index?
A. db.foo.find( { a : 3 } )
A. db.foo.find( { c : 1 } ).sort( { a : 1, b : 1 } )

// --------- How large is your index?

Working set: It's the portion of data that clients are frequently accessing and is located in memory. The key of it is the index, so it's important that our indexes fit into memory.

db.students.stats();
db.students.totalIndexSize();

Wired tiger storage engine may use prefix compression to thinner indexes when compared with MMAP.

// --------- Number of index entries

https://docs.mongodb.com/manual/reference/method/cursor.showRecordId/?_ga=2.126767821.1114842536.1499002021-1068873678.1493258930#cursor.showRecordId

In this lecture, we talk about the cost of moving documents, in terms of updating index entries. That cost only exists in the MMAPv1 storage engine. In the WiredTiger storage engine, index entries don't contain pointers to actual disk locations. Instead, in WiredTiger, the index points to an internal document identifier (the RecordId) that is immutable. Therefore, when a document is updated, its index does not need to be updated at all.

Index cardinality: How many index points are there for each different type of index that MongoDB supports:
	1 - Regular: For every single key that you put in the index, there's certainly going to be an index point. And in addition, if there is no key, then there's going to be an index point under the null entry ----> 1:1
	2 - Sparse: When a document is missing the key being indexed, it's not in the index because it's a null and we don't keep nulls in the index for a sparse index. ----> <= number of documents
	3 - Multikey: It's a index on an array value. Then there may be multiple index points for each document. For example... If there is a tag which has 4 elements, then there's going to be an index point for every single one these keys/elements ----> > number of documents

Let's say you update a document with a key called tags and that update causes the document to need to get moved on disk. Assume you are using the MMAPv1 storage engine. If the document has 100 tags in it, and if the tags array is indexed with a multikey index, how many index points need to be updated in the index to accommodate the move? Put just the number below.
A. 100	

// --------- Geospatial indexes

Obs.: This lecture uses the now deprecated ensureIndex shell command. The preferred command is createIndex. Anywhere you see me using ensureIndex imagine I used createIndex.

Geospatial indexes allow us to find things based on location (it can be any field, but must match its shape).

https://docs.mongodb.com/manual/reference/operator/query-geospatial/

'location' : [x, y]
createIndex({ 'location' : '2d', 'type': 1 }) // 2d tells the database that this is a two-dimensional geospatial index
find({ 'location' : { $near : [x, y] }}).limit(20) // Sample: all the shops that were closest to this "person" standing at coordinates x and y limiting to 20 of them.

Suppose you have a 2D geospatial index defined on the key location in the collection places. Write a query that will find the closest three places (the closest three documents) to the location 74, 140.
A. db.places.find({ 'location' : { $near : [74, 140] }}).limit(3)

// --------- Geospatial spherical

2dsphere is a type of index for it.
MongoDB uses a location specification called GeoJSON.

db.places.createIndex({ 'location' : '2dsphere'})
db.places.getIndexes()
db.places.find({
	location : {
		$near : {
			$geometry : {
				type : "Point",
				coordinates : [-122.166641, 37.4278925]
			},
			$maxDistance : 2000
		}
	}	
})

What is the query that will query a collection named "stores" to return the stores that are within 1,000,000 meters of the location latitude=39, longitude=-130? Type the query in the box below. Assume the stores collection has a 2dsphere index on "loc" and please use the "$near" operator. Each store record looks like this:

{
    "_id":{
        "$oid":"535471aaf28b4d8ee1e1c86f"
    },
    "store_id":8,
    "loc":{
        "type":"Point",
        "coordinates":[
            -37.47891236119904,
            4.488667018711567
        ]
    }
}

A. db.stores.find({ 'loc' : { $near : { $geometry : { 'type' : 'Point', 'coordinates' : [-130,39]}, $maxDistance : 1000000}}})

// --------- Text indexes

Full text search index...

https://docs.mongodb.com/manual/core/index-text/
https://docs.mongodb.com/manual/reference/operator/query/text/

db.sentences.createIndex({ 'words' : 'text' }) // it enables full text search
db.senteces.find({ $text : { $search : 'dog' }})

https://docs.mongodb.com/manual/reference/operator/aggregation/meta/#exp._S_meta

db.senteces.find({ $text : { $search : 'dog' }}, { 'score' : { $meta : 'textScore' }}).sort({ 'score' : { $meta : 'textScore'}})

// -- Quiz

You create a text index on the "title" field of the movies collection, and then perform the following text search:

> db.movies.find( { $text : { $search : "Big Lebowski" } } )

Which of the following documents will be returned, assuming they are in the movies collection? Check all that apply.
A. { "title" : "The Big Lebowski" , star: "Jeff Bridges" }
A. { "title" : "Big" , star : "Tom Hanks" }
A. { "title" : "Big Fish" , star: "Ewan McGregor" }

// --------- Efficiency of Index Use / Designing/Using Indexes

Main goal: Efficient read/write operations
One good way to achieve it is to test your indexes under some real world workloads and make adjustments from there.
Key goals:
	- Selectivity - Minimize records scanned

https://docs.mongodb.com/manual/reference/method/cursor.hint/	

db.students.find({student_id:{$gt:500000}, class_id:54}).sort({student_id:1}).hint({class_id:1}).explain("executionstats")

// -- Quiz

1 - In general, based on the preceding lecture, what is the primary factor that determines how efficiently an index can be used?
A. The selectivity of the index.
WHY: Selectivity is the primary factor that determines how efficiently an index can be used. Ideally, the index enables us to select only those records required to complete the result set, without the need to scan a substantially larger number of index keys (or documents) in order to complete the query. Selectivity determines how many records any subsequent operations must work with. Fewer records means less execution time.

2 - In general, which of the following rules of thumb should you keep in mind when building compound indexes? Check all that apply. For this question, use the following definitions:

equality field: field on which queries will perform an equality test
sort field: field on which queries will specify a sort
range field: field on which queries perform a range test

A. Equality fields before range fields
A. Sort fields before range fields
A. Equality fields before sort fields

// --------- Logging slow queries

https://docs.mongodb.com/manual/tutorial/manage-the-database-profiler/#profiling-levels

MongoDB automatically logs slows queries of above 100 milliseconds right to the log.

// --------- Profiling

https://docs.mongodb.com/manual/reference/database-profiler/

There are THREE levels for the profiler.
	Level 0: Standard level and by default it is off.
	Level 1: It logs all slow queries which are above 100 milliseconds.
	Level 2: It logs all queries with no constraint --> Used during development general known as debugging feature.
> mongod -dppath /usr/local/var/mongodb --profile 1 --slowms 2
> db.students.find({ 'student_id' : 10000 })
> db.system.profile.find().pretty()
// Find anything which name space (ns) matches students collection and sort by timestamp (ts)
> db.system.profile.find({ ns : /school.students/ }).sort({ ts : 1 })pretty() 
> db.system.profile.find({ millis : { $gt : 1 }}).sort({ ts : 1 })pretty() 

> db.getProfilingLevel()
> db.getProfilingStatus()
// Set profiling level to 1 and log queries which take 4 milliseconds to execute
> db.setProfilingLevel(1, 4)
// Turn everything off
> db.setProfilingLevel(0)

// -- Quiz

Write the query to look in the system profile collection for all queries that took longer than one second, ordered by timestamp descending.

A. db.system.profile.find({ millis : { $gt : 1000 }}).sort({ ts : -1 })

// --------- mongotop

https://docs.mongodb.com/manual/reference/program/mongotop/

Mongotop is named after the Unix Top command. It gives a high level view of where Mongo is spending its time.

// --------- mongostat

https://docs.mongodb.com/manual/reference/program/mongostat/

Mongostat is a performance tuning command and is pretty similar to iostat command from the Unix world.

// -- Quiz

Which of the following statements about mongostat output are true?
A. The getmore column concerns the number of requests per time interval to get additional data from a cursor
A. The faults column appears only in the mmapv1 output

// --------- Sharding

https://docs.mongodb.com/manual/sharding/

Sharding is a technique for splitting up a large collection amongst multiple servers.
Replica set keeps the data in sync across several different instances.

##### Homework 4.1

Suppose you have a collection with the following indexes:

> db.products.getIndexes()
[
    {
        "v" : 1,
        "key" : {
            "_id" : 1
        },
        "ns" : "store.products",
        "name" : "_id_"
    },
    {
        "v" : 1,
        "key" : {
            "sku" : 1
        },
                "unique" : true,
        "ns" : "store.products",
        "name" : "sku_1"
    },
    {
        "v" : 1,
        "key" : {
            "price" : -1
        },
        "ns" : "store.products",
        "name" : "price_-1"
    },
    {
        "v" : 1,
        "key" : {
            "description" : 1
        },
        "ns" : "store.products",
        "name" : "description_1"
    },
    {
        "v" : 1,
        "key" : {
            "category" : 1,
            "brand" : 1
        },
        "ns" : "store.products",
        "name" : "category_1_brand_1"
    },
    {
        "v" : 1,
        "key" : {
            "reviews.author" : 1
        },
        "ns" : "store.products",
        "name" : "reviews.author_1"
    }
]

Which of the following queries can utilize at least one index to find all matching documents, or to sort? Check all that apply.

Note: the text for some answers may wrap; you can ignore the wrapping.

A. db.products.find( { 'brand' : "GE" } ).sort( { price : 1 } )
A. db.products.find( { $and : [ { price : { $gt : 30 } }, { price : { $lt : 50 } } ] } ).sort( { brand : 1 } )

##### Homework 4.2

Suppose you have a collection called tweets whose documents contain information about the created_at time of the tweet and the user's followers_count at the time they issued the tweet. What can you infer from the following explain output?

> db.tweets.explain("executionStats").find( { "user.followers_count" : { $gt : 1000 } } ).limit(10).skip(5000).sort( { created_at : 1 } )
{
    "queryPlanner" : {
        "plannerVersion" : 1,
        "namespace" : "twitter.tweets",
        "indexFilterSet" : false,
        "parsedQuery" : {
            "user.followers_count" : {
                "$gt" : 1000
            }
        },
        "winningPlan" : {
            "stage" : "LIMIT",
            "limitAmount" : 0,
            "inputStage" : {
                "stage" : "SKIP",
                "skipAmount" : 0,
                "inputStage" : {
                    "stage" : "FETCH",
                    "filter" : {
                        "user.followers_count" : {
                            "$gt" : 1000
                        }
                    },
                    "inputStage" : {
                        "stage" : "IXSCAN",
                        "keyPattern" : {
                            "created_at" : -1
                        },
                        "indexName" : "created_at_-1",
                        "isMultiKey" : false,
                        "direction" : "backward",
                        "indexBounds" : {
                            "created_at" : [
                                "[MinKey, MaxKey]"
                            ]
                        }
                    }
                }
            }
        },
        "rejectedPlans" : [ ]
    },
    "executionStats" : {
        "executionSuccess" : true,
        "nReturned" : 10,
        "executionTimeMillis" : 563,
        "totalKeysExamined" : 251120,
        "totalDocsExamined" : 251120,
        "executionStages" : {
            "stage" : "LIMIT",
            "nReturned" : 10,
            "executionTimeMillisEstimate" : 500,
            "works" : 251121,
            "advanced" : 10,
            "needTime" : 251110,
            "needFetch" : 0,
            "saveState" : 1961,
            "restoreState" : 1961,
            "isEOF" : 1,
            "invalidates" : 0,
            "limitAmount" : 0,
            "inputStage" : {
                "stage" : "SKIP",
                "nReturned" : 10,
                "executionTimeMillisEstimate" : 500,
                "works" : 251120,
                "advanced" : 10,
                "needTime" : 251110,
                "needFetch" : 0,
                "saveState" : 1961,
                "restoreState" : 1961,
                "isEOF" : 0,
                "invalidates" : 0,
                "skipAmount" : 0,
                "inputStage" : {
                    "stage" : "FETCH",
                    "filter" : {
                        "user.followers_count" : {
                            "$gt" : 1000
                        }
                    },
                    "nReturned" : 5010,
                    "executionTimeMillisEstimate" : 490,
                    "works" : 251120,
                    "advanced" : 5010,
                    "needTime" : 246110,
                    "needFetch" : 0,
                    "saveState" : 1961,
                    "restoreState" : 1961,
                    "isEOF" : 0,
                    "invalidates" : 0,
                    "docsExamined" : 251120,
                    "alreadyHasObj" : 0,
                    "inputStage" : {
                        "stage" : "IXSCAN",
                        "nReturned" : 251120,
                        "executionTimeMillisEstimate" : 100,
                        "works" : 251120,
                        "advanced" : 251120,
                        "needTime" : 0,
                        "needFetch" : 0,
                        "saveState" : 1961,
                        "restoreState" : 1961,
                        "isEOF" : 0,
                        "invalidates" : 0,
                        "keyPattern" : {
                            "created_at" : -1
                        },
                        "indexName" : "created_at_-1",
                        "isMultiKey" : false,
                        "direction" : "backward",
                        "indexBounds" : {
                            "created_at" : [
                                "[MinKey, MaxKey]"
                            ]
                        },
                        "keysExamined" : 251120,
                        "dupsTested" : 0,
                        "dupsDropped" : 0,
                        "seenInvalidated" : 0,
                        "matchTested" : 0
                    }
                }
            }
        }
    },
    "serverInfo" : {
        "host" : "generic-name.local",
        "port" : 27017,
        "version" : "3.0.1",
        "gitVersion" : "534b5a3f9d10f00cd27737fbcd951032248b5952"
    },
    "ok" : 1
}

A. The query uses an index to determine the order in which to return result documents.
A. The query examines 251120 documents.

##### Homework 4.3

> use blog;
> db.posts.drop();
// I have to broke it in three parts because of the maximum size of the file (16 MB)
> mongoimport -d blog -c posts < posts0.json
> mongoimport -d blog -c posts < posts1.json
> mongoimport -d blog -c posts < posts2.json
// Homepage
> db.posts.createIndex({ 'date' : -1 })
// Permalink page
db.posts.createIndex({ 'permalink' : 1 })
// Using unique is probably better and matches the blog rules
db.posts.createIndex({ 'permalink' : 1 }, { 'unique' : true })
// All the posts from a particular tag and order them by date
db.posts.createIndex({ 'tags' : 1, 'date' : -1 })

##### Homework 4.4

> mongoimport --drop -d m101 -c sysprofile sysprofile.json

Now query the profile data, looking for all queries to the students collection in the database school2, sorted in order of decreasing latency. What is the latency of the longest running operation to the collection, in milliseconds?

> db.profile.find({ns: "school2.students" }, {_id: 0, millis: 1 } ).sort( { millis: -1 } ).limit(3)
{ "millis" : 15820 }
{ "millis" : 12560 }
{ "millis" : 11084 }

A. 15820

########################################
########## WEEK 5 - AGGREGATION FRAMEWORK
#### TOPICS: Goals, The Use Of The Pipeline, Comparison With SQL Facilities.

// --------- Simple aggregation example

https://docs.mongodb.com/manual/reference/method/db.collection.aggregate/

> use agg
> db.products.drop()
> db.products.insertMany([{'name':'iPad 16GB Wifi', 'manufacturer':"Apple", 'category':'Tablets', 'price':499.00},
{'name':'iPad 32GB Wifi', 'category':'Tablets', 'manufacturer':"Apple", 'price':599.00},
{'name':'iPad 64GB Wifi', 'category':'Tablets', 'manufacturer':"Apple", 'price':699.00},
{'name':'Galaxy S3', 'category':'Cell Phones', 'manufacturer':'Samsung','price':563.99},
{'name':'Galaxy Tab 10', 'category':'Tablets', 'manufacturer':'Samsung','price':450.99},
{'name':'Vaio', 'category':'Laptops', 'manufacturer':"Sony", 'price':499.00},
{'name':'Macbook Air 13inch', 'category':'Laptops', 'manufacturer':"Apple", 'price':499.00},
{'name':'Nexus 7', 'category':'Tablets', 'manufacturer':"Google", 'price':199.00},
{'name':'Kindle Paper White', 'category':'Tablets', 'manufacturer':"Amazon", 'price':129.00},
{'name':'Kindle Fire', 'category':'Tablets', 'manufacturer':"Amazon", 'price':199.00}])

// SAMPLE SQL WORLD
> SELECT Manufacturer, COUNT(*) FROM Products GROUP BY Manufacturer;
// NOSQL WORLD
> db.products.aggregate([
	{
		$group : {
			_id : "$manufacturer",
			num_products : { $sum : 1 }
		}
	}
])
// RESULT OF THE AGGREGATE ABOVE:
{ "_id" : "Amazon", "num_products" : 2 }
{ "_id" : "Apple", "num_products" : 4 }
{ "_id" : "Samsung", "num_products" : 2 }
{ "_id" : "Sony", "num_products" : 1 }
{ "_id" : "Google", "num_products" : 1 }

// -- Quiz

Write the aggregation query that will find the number of products by category of a collection that has the form:

{
    "_id" : ObjectId("50b1aa983b3d0043b51b2c52"),
    "name" : "Nexus 7",
    "category" : "Tablets",
    "manufacturer" : "Google",
    "price" : 199
}

Have the resulting key be called "num_products," as in the video lesson. Hint, you just need to change which key you are aggregating on relative to the examples shown in the lesson.

Please double quote all keys to make it easier to check your result.

db.products.aggregate([
	{
		$group : {
			_id : "$category",
			num_products : { $sum : 1 }
		}
	}
])

// --------- Aggregation pipeline

https://docs.mongodb.com/manual/core/aggregation-pipeline/
https://docs.mongodb.com/manual/reference/operator/aggregation/#aggregation-pipeline-operator-reference

Which of the following are stages in the aggregation pipeline. Check all that apply.

A. Match, Group, Skip, Limit, Sort, Project and Unwind.

// --------- Simple aggregation example

// -- Quiz

If you have the following collection of stuff:

> db.stuff.find()
{ "_id" : ObjectId("50b26f9d80a78af03b5163c8"), "a" : 1, "b" : 1, "c" : 1 }
{ "_id" : ObjectId("50b26fb480a78af03b5163c9"), "a" : 2, "b" : 2, "c" : 1 }
{ "_id" : ObjectId("50b26fbf80a78af03b5163ca"), "a" : 3, "b" : 3, "c" : 1 }
{ "_id" : ObjectId("50b26fcd80a78af03b5163cb"), "a" : 3, "b" : 3, "c" : 2 }
{ "_id" : ObjectId("50b26fd380a78af03b5163cc"), "a" : 3, "b" : 5, "c" : 3 }

and you perform the following aggregation:

> db.stuff.aggregate([{$group:{_id:'$c'}}])

How many documents will be in the result set from aggregate?
A. 3


// --------- Compound groupings

Let's suppose we want to find out the number of products that each manufacturer had in each category?

// SAMPLE SQL WORLD
> SELECT manufacturer, category, COUNT(*) FROM products GROUP BY manufacturer, category;
// NOSQL WORLD
> db.products.aggregate([
    {
		$group: {
			_id: { "manufacturer" : "$manufacturer", "category" : "$category"},
			num_products : { $sum : 1 }
		}
    }
])
// RESULT OF THE AGGREGATE ABOVE:
{ "_id" : { "manufacturer" : "Google", "category" : "Tablets" }, "num_products" : 1 }
{ "_id" : { "manufacturer" : "Sony", "category" : "Laptops" }, "num_products" : 1 }
{ "_id" : { "manufacturer" : "Apple", "category" : "Tablets" }, "num_products" : 3 }
{ "_id" : { "manufacturer" : "Samsung", "category" : "Cell Phones" }, "num_products" : 1 }
{ "_id" : { "manufacturer" : "Samsung", "category" : "Tablets" }, "num_products" : 1 }
{ "_id" : { "manufacturer" : "Apple", "category" : "Laptops" }, "num_products" : 1 }
{ "_id" : { "manufacturer" : "Amazon", "category" : "Tablets" }, "num_products" : 2 }

// -- Quiz

Given the following collection:

> db.stuff.find()
{ "_id" : ObjectId("50b26f9d80a78af03b5163c8"), "a" : 1, "b" : 1, "c" : 1 }
{ "_id" : ObjectId("50b26fb480a78af03b5163c9"), "a" : 2, "b" : 2, "c" : 1 }
{ "_id" : ObjectId("50b26fbf80a78af03b5163ca"), "a" : 3, "b" : 3, "c" : 1 }
{ "_id" : ObjectId("50b26fcd80a78af03b5163cb"), "a" : 3, "b" : 3, "c" : 2 }
{ "_id" : ObjectId("50b26fd380a78af03b5163cc"), "a" : 3, "b" : 5, "c" : 3 }
{ "_id" : ObjectId("50b27f7080a78af03b5163cd"), "a" : 3, "b" : 3, "c" : 2 }

And the following aggregation query:

db.stuff.aggregate([
	{
		$group: {
			_id: {'moe':'$a', 'larry':'$b', 'curly':'$c'}
		}
	}
])

How many documents will be in the result set?
A. 5

// --------- Aggregation expressions overview

$sum, $avg, $min, $max, $push, $addToSet, $fisrt, $last ---> For $group stage
$push, $addToSet -> Build arrays.
$fisrt, $last -> Require first to sort the documents or they don't make any sense.

// --------- Using $sum

// -- Quiz

mongoimport -d week5 -c zips < zips.f7b79f61a996.json

Suppose we have a collection of populations by postal code. The postal codes in are in the _id field, and are therefore unique. Documents look like this:

{
    "city" : "CLANTON",
    "loc" : [
        -86.642472,
        32.835532
    ],
    "pop" : 13990,
    "state" : "AL",
    "_id" : "35045"
}

Write an aggregation query to sum up the population (pop) by state and put the result in a field called population. Don't use a compound _id key (you don't need one and the quiz checker is not expecting one). The collection name is zips. so something along the lines of db.zips.aggregate...

A. > db.zips.aggregate([
	{
		$group : {
			_id : "$state",
			population : { $sum : "$pop" }
		}
	}
])

// --------- Using $avg

// -- Quiz

Given population data by zip code (postal code) that looks like this:

{
    "city" : "FISHERS ISLAND",
    "loc" : [
            -72.017834,
            41.263934
    ],
    "pop" : 329,
    "state" : "NY",
    "_id" : "06390"
}

Write an aggregation expression to calculate the average population of a zip code (postal code) by state. As before, the postal code is in the _id field and is unique. The collection is assumped to be called "zips" and you should name the key in the result set "average_pop"

A. > db.zips.aggregate([
	{
		$group : {
			_id : "$state",
			average_pop : { $avg : "$pop" }
		}
	}
])

// --------- Using $addToSet

// -- Quiz

Suppose we population by zip code (postal code) data that looks like this (putting in a query for the zip codes in Palo Alto)

> db.zips.find({state:"CA",city:"PALO ALTO"})
{ "city" : "PALO ALTO", "loc" : [ -122.149685, 37.444324 ], "pop" : 15965, "state" : "CA", "_id" : "94301" }
{ "city" : "PALO ALTO", "loc" : [ -122.184234, 37.433424 ], "pop" : 1835, "state" : "CA", "_id" : "94304" }
{ "city" : "PALO ALTO", "loc" : [ -122.127375, 37.418009 ], "pop" : 24309, "state" : "CA", "_id" : "94306" }

Write an aggregation query that will return the postal codes that cover each city. The results should look like this:

{
    "_id" : "CENTREVILLE",
    "postal_codes" : [
        "22020",
        "49032",
        "39631",
        "21617",
        "35042"
    ]
},

Again the collection will be called zips. You can deduce what your result column names should be from the above output. (ignore the issue that a city may have the same name in two different states and is in fact two different cities in that case - for eg Springfield, MO and Springfield, MA)

A. > db.zips.aggregate([
	{
		$group : {
			_id : "$city",
			postal_codes : { $addToSet : "$_id" }
		}
	}
])

// --------- Using $push

// -- Quiz

Given the zipcode dataset (explained more fully in the using $sum quiz) that has documents that look like this:

> db.zips.findOne()
{
    "city" : "ACMAR",
    "loc" : [
        -86.51557,
        33.584132
    ],
    "pop" : 6055,
    "state" : "AL",
    "_id" : "35004"
}

would you expect the following two queries to produce the same result or different results?

db.zips.aggregate([{"$group":{"_id":"$city", "postal_codes":{"$push":"$_id"}}}])
db.zips.aggregate([{"$group":{"_id":"$city", "postal_codes":{"$addToSet":"$_id"}}}])

A. SAME because _id field is unique, thus doesn't matter if you use push or addToSet, the result will be the same.

// --------- Using $max and $min

// -- Quiz

Again thinking about the zip code database, write an aggregation query that will return the population of the postal code in each state with the highest population. It should return output that looks like this:

{
	"_id" : "WI",
	"pop" : 57187
},
{
	"_id" : "WV",
	"pop" : 70185
},
..and so on

A. > db.zips.aggregate([
	{
		$group : {
			_id : "$state",
			pop : { $max : "$pop" }
		}
	}
])

// --------- Double $group stages

// -- Quiz

Given the following collection:

> db.fun.find()
{ "_id" : 0, "a" : 0, "b" : 0, "c" : 21 }
{ "_id" : 1, "a" : 0, "b" : 0, "c" : 54 }
{ "_id" : 2, "a" : 0, "b" : 1, "c" : 52 }
{ "_id" : 3, "a" : 0, "b" : 1, "c" : 17 }
{ "_id" : 4, "a" : 1, "b" : 0, "c" : 22 }
{ "_id" : 5, "a" : 1, "b" : 0, "c" : 5 }
{ "_id" : 6, "a" : 1, "b" : 1, "c" : 87 }
{ "_id" : 7, "a" : 1, "b" : 1, "c" : 97 }

And the following aggregation query:

db.fun.aggregate([
	{
		$group : {
			_id : {a:"$a", b:"$b"}, 
			c : {$max:"$c"}
		}
	}, 
	{
		$group : {
			_id : "$_id.a", 
			c : { $min:"$c" }
		}
	}
])

What values are returned?

A. db.fun.insertMany([{ "_id" : 0, "a" : 0, "b" : 0, "c" : 21 },
{ "_id" : 1, "a" : 0, "b" : 0, "c" : 54 },
{ "_id" : 2, "a" : 0, "b" : 1, "c" : 52 },
{ "_id" : 3, "a" : 0, "b" : 1, "c" : 17 },
{ "_id" : 4, "a" : 1, "b" : 0, "c" : 22 },
{ "_id" : 5, "a" : 1, "b" : 0, "c" : 5 },
{ "_id" : 6, "a" : 1, "b" : 1, "c" : 87 },
{ "_id" : 7, "a" : 1, "b" : 1, "c" : 97 }])

When the aggregation query run it returns:
{ "_id" : 0, "c" : 52 }
{ "_id" : 1, "c" : 22 }

// --------- Using $project

The project phase lets you reshape the documents as they come through the pipeline.

// -- Quiz

Write an aggregation query with a single projection stage that will transform the documents in the zips collection from this:

{
    "city" : "ACMAR",
    "loc" : [
        -86.51557,
        33.584132
    ],
    "pop" : 6055,
    "state" : "AL",
    "_id" : "35004"
}

to documents in the result set that look like this:

{
    "city" : "acmar",
    "pop" : 6055,
    "state" : "AL",
    "zip" : "35004"
}

So that the checker works properly, please specify what you want to do with the _id key as the first item. The other items should be ordered as above. As before, assume the collection is called zips. You are running only the projection part of the pipeline for this quiz.

Obs.: If you want to include a key exactly as it is named in the source document, you just write key:1, where key is the name of the key.

A. > db.zips.aggregate([
	{
		$project : {
			_id : 0,
			city : { $toLower: "$city" },
			pop : 1,
			state : 1,
			zip : "$_id"
		}
	}
])

// --------- Using $match

// -- Quiz

Again, thinking about the zipcode collection, write an aggregation query with a single match phase that filters for zipcodes with greater than 100,000 people. You may need to look up the use of the $gt operator in the MongoDB docs. Assume the collection is called zips.

A. > db.zips.aggregate([
	{
		$match : {
			pop : { $gt : 100000 }
		}
	}
])

// --------- Using $sort

// -- Quiz

Again, considering the zipcode collection, which has documents that look like this,

{
    "city" : "ACMAR",
    "loc" : [
        -86.51557,
        33.584132
    ],
    "pop" : 6055,
    "state" : "AL",
    "_id" : "35004"
}

Write an aggregation query with just a sort stage to sort by (state, city), both ascending. Assume the collection is called zips.

A. > db.zips.aggregate([
	{
		$sort : {
			state: 1,
			city: 1
		}
	}
])

// --------- Using $limit and $skip

// -- Quiz

Suppose you change the order of skip and limit in the query shown in the lesson, to look like this:

db.zips.aggregate([
    { $match: { state:"NY" } },
    { $group: { _id: "$city", population: {$sum:"$pop"}} },
    { $project: { _id: 0, city: "$_id", population: 1} },
    { $sort: { population:-1 } },
    {$limit: 5},
    {$skip: 10}
])

How many documents do you think will be in the result set?

A. 0

// --------- Revisiting $first and $last

// -- Quiz

Given the following collection:

> db.fun.find()
{ "_id" : 0, "a" : 0, "b" : 0, "c" : 21 }
{ "_id" : 1, "a" : 0, "b" : 0, "c" : 54 }
{ "_id" : 2, "a" : 0, "b" : 1, "c" : 52 }
{ "_id" : 3, "a" : 0, "b" : 1, "c" : 17 }
{ "_id" : 4, "a" : 1, "b" : 0, "c" : 22 }
{ "_id" : 5, "a" : 1, "b" : 0, "c" : 5 }
{ "_id" : 6, "a" : 1, "b" : 1, "c" : 87 }
{ "_id" : 7, "a" : 1, "b" : 1, "c" : 97 }

What would be the value of c in the result from this aggregation query

db.fun.aggregate([
    {$match:{a:0}},
    {$sort:{c:-1}},
    {$group:{_id:"$a", c:{$first:"$c"}}}
])

A. 54

// --------- Using $unwind

// -- Quiz

Suppose you have the following collection:

db.people.find()
{ "_id" : "Will", "likes" : [ "physics", "MongoDB", "indexes" ] }
{ "_id" : "Dwight", "likes" : [ "starting companies", "restaurants", "MongoDB" ] }

And you unwind the "likes" array of each document. How many documents will you wind up with?

A. 6

// --------- $unwind example

// -- Quiz

Which grouping operator will enable to you to reverse the effects of an unwind?

A. $push

// --------- Double $unwind

// -- Quiz

Can you reverse the effects of a double unwind (2 unwinds in a row) in our inventory collection (shown in the lesson ) with the $push operator?

A. Yes

// --------- Mapping between SQL and Aggregation

https://docs.mongodb.com/manual/reference/sql-aggregation-comparison/

// --------- Limitations of the Aggregation Framework

- 100 MB limit for pipeline stage. You can increase that using allowDiskUse option.
- 16 MB limit by default in Python
- Sharded -> When use something that requires looking all the data like $match or $sort, MongoDB is going to send all that data to the primary shard in order to everything be collected in one place. HADDOUP is the recommened guy for it.

##### Homework 5.1

In this assignment you will use the aggregation framework to find the most frequent author of comments on your blog.

> mongoimport -d blog -c posts < posts0.json
> mongoimport -d blog -c posts < posts1.json
> mongoimport -d blog -c posts < posts2.json

Now use the aggregation framework to calculate the author with the greatest number of comments.
To help you verify your work before submitting, the author with the fewest comments is Mariela Sherer and she commented 387 times.


> db.posts.aggregate([
	{
		$unwind : "$comments"
	},
	{
		$group : {
			_id : "$comments.author",
			sum_author : { $sum : 1 }
		}
	},
	{
		$sort : {
			sum_author : -1
		}
	}
])
A. { "_id" : "Elizabet Kleine", "sum_author" : 503 }

##### Homework 5.2

Please calculate the average population of cities in California (abbreviation CA) and New York (NY) (taken together) with populations over 25,000. For this problem, assume that a city name that appears in more than one state represents two separate cities. Please round the answer to a whole number.

Hint: The answer for CT and NJ (using this data set) is 38177.

Please note: Different states might have the same city name; A city might have multiple zip codes.

For this problem, we have used a subset of the data you previously used in zips.json, not the full set. For this set, there are only 200 documents (and 200 zip codes), and all of them are in New York, Connecticut, New Jersey, and California.
You can download the handout and perform your analysis on your machine with

> mongoimport --drop -d test -c zips small_zips.28ecb7fb8fba.json
> db.zips.aggregate([
	{
		$group : {
			_id : { state : "$state", city: "$city" },
			pop : { $sum : "$pop" }
		}
	},
	{
		$match : {
			'_id.state' : { $in : ['CA', 'NY'] },
			pop : { $gt : 25000 }
		}
	},
	{
		$group : {
			_id : null,
			avg_pop : { $avg : "$pop" }
		}	
	}
])
A. 44805

##### Homework 5.3

Who's the easiest grader on campus? Download the handout and mongoimport. The documents look like this:

{
    "_id" : ObjectId("50b59cd75bed76f46522c392"),
    "student_id" : 10,
    "class_id" : 5,
    "scores" : [
        {
            "type" : "exam",
            "score" : 69.17634380939022
        },
        {
            "type" : "quiz",
            "score" : 61.20182926719762
        },
        {
            "type" : "homework",
            "score" : 73.3293624199466
        },
        {
            "type" : "homework",
            "score" : 15.206314042622903
        },
        {
            "type" : "homework",
            "score" : 36.75297723087603
        },
        {
            "type" : "homework",
            "score" : 64.42913107330241
        }
    ]
}

There are documents for each student (student_id) across a variety of classes (class_id). Note that not all students in the same class have the same exact number of assessments. Some students have three homework assignments, etc.

Your task is to calculate the class with the best average student performance. This involves calculating an average for each student in each class of all non-quiz assessments and then averaging those numbers to get a class average. To be clear, each student's average includes only exams and homework grades. Don't include their quiz scores in the calculation.

What is the class_id which has the highest average student performance?

Hint/Strategy: You need to group twice to solve this problem. You must figure out the GPA that each student has achieved in a class and then average those numbers to get a class average. After that, you just need to sort. The class with the lowest average is the class with class_id=2. Those students achieved a class average of 37.6.


> mongoimport --drop -d test -c grades grades.json
> db.grades.aggregate([
	{ $unwind : "$scores" },
	{ $match : { "scores.type" : { "$ne" : "quiz" }}},
	{
	  $group : {
		"_id": { class_id : "$class_id", student_id : "$student_id"},
		avg_per_student : { "$avg" : "$scores.score"}
	}
	},
	{
	  $group : {
		_id : "$_id.class_id",
		avg: { "$avg" : "$avg_per_student" }
	}
	},
	{ $sort: { "avg" : -1 }}
])
A. 1

##### Homework $substr

In this problem you will calculate the number of people who live in a zip code in the US where the city starts with one of the following characthers:

B, D, O, G, N or M .

We will take these are the prefered cities to live in (chosen by this instructor, given is special affection to this set of characters!). You will be using the zip code collection data set, which you will find in the 'handouts' link in this page. The $project stage can extract the first character from any field. For example, to extract the first character from the city field, you could write this pipeline:

> db.zips.aggregate([
	{ 
		$project: { 
			first_char: { $substr : [ "$city", 0, 1 ] }
		}
	}
])

Using the aggregation framework, calculate the sum total of people who are living in a zip code where the city starts with one of those possible first characters. Choose the answer below.

You will need to probably change your projection to send more info through than just that first character. Also, you will need a filtering step to get rid of all documents where the city does not start with the select set of initial characters.

> mongoimport --drop -d test -c zips zips.f7b79f61a996.json
> db.zips.aggregate([
	{
		$project : {
			first_char : { $substr : [ "$city", 0, 1 ] },
			pop : 1
		}
	},
	{ 
		$match : {
			"first_char" : { "$regex": "[BDOGNM]" } 
		}
	},
	{
		$group : {
			"_id" : null,
			sum : { "$sum" : "$pop" }
		}
	}
])

A. 76394871

########################################
########## WEEK 6 - APPLICATION ENGINEERING
#### TOPICS: Drivers, Impact Of Replication And Sharding On Design And Development.

// --------- Write concern

https://docs.mongodb.com/manual/reference/write-concern/

Write concern describes the level of acknowledgement requested from MongoDB for write operations to a standalone mongod or to replica sets or to sharded clusters. In sharded clusters, mongos instances will pass the write concern on to the shards.

1 - By default, W equals 1 and J equals false, it means that we're going to wait for that write to be acknowledged by the database, but we're not going to wait for the jornal to sync => This is FAST, but there is a small window of vunerability...
2 - If we want to eliminate the "small window of vunerability", we can set w as 1 an J as true => This is SLOW, but secure.

// -- Quiz

Provided you assume that the disk is persistent, what are the w and j settings required to guarantee that an insert or update has been written all the way to disk?

A. w=1, j=1

// --------- Network Errors

When a insert uses a custom _id, then it's impossible to get some problem about the consequences of network errors, but the main problem resides on update statements. $inc command is a way of workaround it.

https://docs.mongodb.com/manual/reference/operator/update/inc/

// -- Quiz

What are the reasons why an application may receive an error back even if the write was successful. Check all that apply.

A. The network TCP connection between the application and the server was reset after the server received a write but before a response could be sent.
A. The MongoDB server terminates between receiving the write and responding to it.
A. The network fails between the time of the write and the time the client receives a response to the write.

// --------- Introduction to Replication

https://docs.mongodb.com/manual/replication/

We must be able to use the system even if something unlikely happens. Two objectives

- Availability
- Fault tolerance

In order to get both above there is a thing called Replica Set. It is a set of Mongo nodes and all mirror each other in terms of the data. There is one primary and the other nodes are secondaries, but that's dynamic.

Data that's written to the primary will asynchronously replicate to the secondaries.

Your application stays connected to the primary, it can only write on it.

- If the primary goes down, then the remaining nodes will perform an election to elect a new primary.
- Your application will reconnect to the new primary.
- If the fallen one returns, then it is allocated as secondary.
- The minimum number of nodes is 3

// -- Quiz

What is the minimum original number of nodes needed to assure the election of a new Primary if a node goes down?

A. 3

// --------- Replica set elections

Types of replica sets:

1 - Regular: Can be primary os secondary
2 - Arbiter: It's just there for voting purpose. So you can have two regular nodes and one arbiter to have a strict majority to vote a new leader (primary).
3 - Delayed: It can be one, two hours behind the other nodes. It can participate in the voting too, but it cannot become a primary node. Priority set to 0.
4 - Hidden: It's often used for analytics. It cannot become the primary node. Priority set to 0.

// -- Quiz

Which types of nodes can participate in elections of a new primary?

A. Regular, hidden and arbiters.

// --------- Write consistency

Your writes always go to the primary but your reads don't have to go to the primary. The could go to the secondaries.

As the writes/redas go to the primary, you get strong consistency of reads with respect to writes => You won't read stale data.
If you go to the secondaries, you may get stale data.

https://docs.mongodb.com/manual/core/read-isolation-consistency-recency/

// -- Quiz

During the time when failover is occurring, can writes successfully complete?

A. NO

// --------- Creating a replica set

Even after the start of create_replica_set__m101p_52b35df6e2d423678d3b9d48.19830756d7a7.sh, it's needed to create a configuration because each instance of mongod does not know about each other, so you must tell them, they're basically independent. 

mongod --replSet m101 --logpath "1.log" --dbpath /data/rs1 --port 27017 --oplogSize 64 --fork --smallfiles
mongod --replSet m101 --logpath "2.log" --dbpath /data/rs2 --port 27018 --oplogSize 64 --smallfiles --fork
mongod --replSet m101 --logpath "3.log" --dbpath /data/rs3 --port 27019 --oplogSize 64 --smallfiles --fork

The following command in mongo shell will do the trick:

config = { _id: "m101", members:[
          { _id : 0, host : "localhost:27017", priority:0, slaveDelay: 5},
          { _id : 1, host : "localhost:27018"},
          { _id : 2, host : "localhost:27019"} ]
};

rs.initiate(config);
rs.status();

You important thing to notice is that you cannot use this command in a host which cannot become the primary node like localhost:27017.

> mongo --port 27018 < init_replica.js
> rs.status() // In order to see if it's everything configured or not
> rs.slaveOk() // Informing that you're OK with reading from a secondary

// -- Quiz

Which command, when issued from the mongo shell, will allow you to read from a secondary?

A. rs.slaveOk()

// --------- Replica set internals

https://www.mongodb.com/presentations/replication-internals-life-write-0
https://github.com/mongodb/mongo/wiki/Replication-Internals
https://docs.mongodb.com/manual/core/replica-set-oplog/

The oplog (operations log) is a special capped collection that keeps a rolling record of all operations that modify the data stored in your databases. MongoDB applies database operations on the primary and then records the operations on the primary’s oplog. The secondary members then copy and apply these operations in an asynchronous process. All replica set members contain a copy of the oplog, in the local.oplog.rs collection, which allows them to maintain the current state of the database.

> use local
> show collections
> db.oplog.rs.find() // Read from primary and secondaries nodes

The oplog from secondary has the same statements (creation of collections and documents) as the primary.

"optime" and "optimeDate" have the information to the secondary that he has everything up to date to this point.

> ps -ef | mongod
> kill <primary node PID>
// Issue the command below from the new primary
> rs.status()
// -- Quiz

Which of the following statements are true about replication. Check all that apply.

A. Replication supports mixed-mode storage engines. For examples, a mmapv1 primary and wiredTiger secondary.
A. A copy of the oplog is kept on both the primary and secondary servers.
A. The oplog is implemented as a capped collection.

// --------- Failover and rollback

https://docs.mongodb.com/manual/core/replica-set-rollbacks/

// -- Quiz

What happens if a node comes back up as a secondary after a period of being offline and the oplog has looped on the primary?

A. The entire dataset will be copied from the primary

// --------- Connecting to a Replica Set from the Java Driver

// -- Quiz

If you leave a replica set node out of the seedlist within the driver, what will happen?

A. The missing node will be discovered as long as you list at least one valid node.

Obs: If that node happens to be one that's been taken down for maintenance, the discovery won't work.

// --------- When Bad Things Happen to Good Nodes

// When testing the replica set, you can shut down one node using the folling command:
> rs.stepDown()

// -- Quiz

If you use the MongoClient constructor that takes a seed list of replica set members, are you guaranteed to avoid application exceptions during a primary failover?

A. No.

// --------- Write concern revisited

Write concern (w) value can be set at client, database or collection level within PyMongo. When you call MongoClient, you get a connection to the driver, but behind the scenes, PyMongo connects to multiple nodes of the replica set. The w value can be set at the client level. Andrew says that the w concern can be set at the connection level; he really means client level. It's also important to note that wtimeout is the amount of time that the database will wait for replication before returning an error on the driver, but that even if the database returns an error due to wtimeout, the write will not be unwound at the primary and may complete at the secondaries. Hence, writes that return errors to the client due to wtimeout may in fact succeed, but writes that return success, do in fact succeed. Finally, the video shows the use of an insert command in PyMongo. That call is deprecated and it should have been insert_one.

At minute 3:01, Andrew mentions that j:true does not wait for secondary node. That is not longer true. Using replication protocol V1, introduced in 3.2, j:true does wait for journal acknowledgement on secondaries

https://docs.mongodb.com/manual/reference/write-concern/?_ga=2.191709802.1108080235.1499469715-1068873678.1493258930#j-option

> rs.conf() // In order to see which one is running in a delayed state
> rs.help()

// -- Quiz

If you set w=1 and j=1, is it possible to wind up rolling back a committed write to the primary on failover?

A. No.

// --------- Read preferences

The newest version of the pymongo API docs can be found here:
http://api.mongodb.com/python/current/api/pymongo/?_ga=2.150948598.1108080235.1499469715-1068873678.1493258930

Read preference describes how MongoDB clients route read operations to the members of a replica set. By default, an application directs its read operations to the primary member in a replica set, but you can configure to read from the nearest node, which can be a secondary one.
https://docs.mongodb.com/manual/core/read-preference/

By default your writes and reads are going to go to you primary. And that's a good thing, because as a result, you're going to read what you wrote. Because with replication, it maybe the case that if you did a read to a secondary and had written it to the primary, if the write had not yet propagated to the seconday, you may not read what you wrote, which makes it harder to reason about your programs.

Options available:

- primary (Default mode): All operations read from the current replica set primary.
- primaryPreferred: In most situations, operations read from the primary but if it is unavailable, operations read from secondary members.
- secondary: All operations read from the secondary members of the replica set.
- secondaryPreferred: In most situations, operations read from secondary members but if no secondary members are available, operations read from the primary.
- nearest: Operations read from member of the replica set with the least network latency, irrespective of the member’s type.

// -- Quiz

You can configure your applications via the drivers to read from secondary nodes within a replica set. What are the reasons that you might not want to do that? Check all that apply.

A. If your write traffic is great enough, and your secondary is less powerful than the primary, you may overwhelm the secondary, which must process all the writes as well as the reads. Replication lag can result.
	- This is a design anti-pattern that we sometimes see.
	- A similar anti-pattern occurs when reads are routed to the primary, but the secondary is underpowered and unable to handle the full read + write load. In this case, if the secondary becomes primary, it will be unable to fulfill its job.
A. You may not read what you previously wrote to MongoDB on a secondary because it will lag behind by some amount.
	- This is pretty straightforward. Unless you are reading from the primary, the secondary will not necessarily have the most current version of the documents you need to read.
	- Whether this is a problem or not depends on your application's requirements and business concerns, so it goes a bit outside the scope of development.
A. If the secondary hardware has insufficient memory to keep the read working set in memory, directing reads to it will likely slow it down.
	- This could really go either way. If the secondary has excess capacity, beyond what it needs to take writes, then directing reads to it would cause it to work more, but perhaps it would still be able to keep up with the oplog. On the other hand, if the primary is taking writes faster than the secondary can keep up, then this scenario would definitely slow it down.
	- Generally, your secondary should be on the same hardware as your primary, so if that's the case, and your primary would be able to keep up with the reads, then this shouldn't be a problem. Of course, if your primary can handle both the read and write loads, then there's really no compelling reason to send the reads to the secondary.

// --------- Review implications of replication

One thing to remember is that the driver will check, upon attempting to write, whether or not its write concern is valid. It will error if, for example, w=4 but there are 3 data-bearing replica set members. This will happen quickly in both the Java and pymongo drivers. Reading with an invalid readPreference will take longer, but will also result in an error. Be aware, though, that this behavior can vary a little between drivers and between versions.

// -- Quiz

If you set w=4 on a MongoClient and there are only three nodes in the replica set, how long will you wait in PyMongo for a response from an insert if you don't set a timeout?

A. You will get an immediate error

// --------- Introduction to sharding

Sharding is Mongo's approach to horizontal scalability.

At 2:55 I say that we use a range-based approach to distributing data across shards. That was right when this lecture was recorded but no longer. As of MongoDB 2.4, we also offer hash-based sharding, which offers a more even distribution of data as a function of shard key, at the expense of worse performance for range-based queries. For more information, see the documentation at http://docs.mongodb.org/manual/core/sharding-introduction/

The shard_key is some part of the document itself.

When it's set up, instead of connecting mongo shell to a replica set, you'll connect to a MongoS Router.

// -- Quiz

If the shard key is not included in a find operation and there are 4 shards, each one a replica set with 3 nodes, how many nodes will see the find operation?

A. 4. Since the shard key is not included in the find operation, mongos has to send the query to all 4 of the shards. Each shard has 3 replica-set members, but only one member of each replica set (the primary, by default) is required to handle the find.


// --------- Building a sharded environment

https://docs.mongodb.com/manual/tutorial/deploy-shard-cluster/

> mongo
// The command below gives you the status of a sharded system.
> sh.status()

// -- Quiz

If you want to build a production system with two shards, each one a replica set with three nodes, how may mongod processes must you start?

A. 9 because 6 of them are going to come from the shards, there's two replica sets each one with three nodes and there's  three config servers that's what we recommend from production system.

// --------- Implication of sharding on development

- Every document needs to include the shard key
- Shard key is immutable
- You need an index that starts with the shard key
- Shard key specified
- When you do an update, you have to specify the shard key or specify that multi is true
- Whatever you do a query that doesn't include the shard key, it's going to sent it across all the nodes
- Document can't have an unique key, unless it's also part of the shard key

// -- Quiz

Suppose you wanted to shard the zip code collection after importing it. You want to shard on zip code. What index would be required to allow MongoDB to shard on zip code?

A. An index on zip or a non-multi-key index that starts with zip code.

// --------- Sharding + Replication

Write concern is still applied even with sharding as it has replica set inside of it.

// -- Quiz

Suppose you want to run multiple mongos routers for redundancy. What level of the stack will assure that you can failover to a different mongos from within your application?

A. Drivers.

// --------- Choosing a shard key

https://docs.mongodb.com/manual/core/sharding-shard-key/#choosing-a-shard-key
http://techinsides.blogspot.com.br/2013/09/keynote-concerns-how-to-choose-mongodb.html

1 - Sufficient cardinality: It means sufficient variety of values. Like you decide to shard a field where there's only three possible values, then would be no way for Mongo to spread it for exmaple across 100 shards.
2 - The shard key should enable us to avoid "hot-spotting" of write operations. "Hot-spotting" of writes is said to occur when all writes, one after another start landing on the same shard.

// -- Quiz

You are building a facebook competitor called footbook that will be a mobile social network of feet. You have decided that your primary data structure for posts to the wall will look like this:

{	
	'username' : 'toeguy',
	'posttime' : ISODate("2012-12-02T23:12:23Z"),
	"randomthought" : "I am looking at my feet right now",
	'visible_to' : ['friends','family', 'walkers']
}
	 
Thinking about the tradeoffs of shard key selection, select the true statements below.

A. Choosing posttime as the shard key  will cause hotspotting as time progresses.
A. Choosing username as the shard key will distribute posts to the wall well across the shards.
A. Choosing visible_to as a shard key is illegal.

##### Homework 6.1

Which of the following statements are true about replication in MongoDB? Check all that apply.

A. The minimum sensible number of voting nodes to a replica set is three.
A. The oplog utilizes a capped collection.

##### Homework 6.2

Let's suppose you have a five member replica set and want to assure that writes are committed to the journal and are acknowledged by at least 3 nodes before you proceed forward. What would be the appropriate settings for w and j?

A. w="majority", j=1

##### Homework 6.3

Which of the following statements are true about choosing and using a shard key?

A. There must be a index on the collection that starts with the shard key.
A. MongoDB can not enforce unique indexes on a sharded collection other than the shard key itself, or indexes prefixed by the shard key.
A. Any update that does not contain the shard key or _id field will result in an error.

##### Homework 6.4

You have a sharded system with three shards and have sharded the collections "students" in the "school" database across those shards. The output of sh.status() when connected to mongos looks like this:

> sh.status()

--- Sharding Status ---
  sharding version: {
    "_id" : 1,
    "minCompatibleVersion" : 5,
    "currentVersion" : 6,
    "clusterId" : ObjectId("5531512ac723271f602db407")
}
  shards:
    {  "_id" : "s0",  "host" : "s0/localhost:37017,localhost:37018,localhost:37019" }
    {  "_id" : "s1",  "host" : "s1/localhost:47017,localhost:47018,localhost:47019" }
    {  "_id" : "s2",  "host" : "s2/localhost:57017,localhost:57018,localhost:57019" }
  balancer:
    Currently enabled:  yes
    Currently running:  yes
        Balancer lock taken at Fri Apr 17 2015 14:32:02 GMT-0400 (EDT) by education-iMac-2.local:27017:1429295401:16807:Balancer:1622650073
    Collections with active migrations:
        school.students started at Fri Apr 17 2015 14:32:03 GMT-0400 (EDT)
    Failed balancer rounds in last 5 attempts:  0
    Migration Results for the last 24 hours:
        2 : Success
        1 : Failed with error 'migration already in progress', from s0 to s1
  databases:
    {  "_id" : "admin",  "partitioned" : false,  "primary" : "config" }
    {  "_id" : "school",  "partitioned" : true,  "primary" : "s0" }
        school.students
            shard key: { "student_id" : 1 }
            chunks:
                s0  1
                s1  3
                s2  1
            { "student_id" : { "$minKey" : 1 } } -->> { "student_id" : 0 } on : s2 Timestamp(3, 0)
            { "student_id" : 0 } -->> { "student_id" : 2 } on : s0 Timestamp(3, 1)
            { "student_id" : 2 } -->> { "student_id" : 3497 } on : s1 Timestamp(3, 2)
            { "student_id" : 3497 } -->> { "student_id" : 7778 } on : s1 Timestamp(3, 3)
            { "student_id" : 7778 } -->> { "student_id" : { "$maxKey" : 1 } } on : s1 Timestamp(3, 4)

If you ran the query:

> use school
> db.students.find({'student_id':2000})

Which shards would be involved in answering the query?

A. S1

##### Homework 6.5

In this homework you will build a small replica set on your own computer. We will check that it works with MongoProc.

Create three directories for the three mongod processes. On unix, this could be done as follows:

> mkdir -p /data/rs1 /data/rs2 /data/rs3

Or on Windows:

> mkdir \data\rs1 \data\rs2 \data\rs3

Now start three mongo instances as follows. Note that are three commands. The browser is probably wrapping them visually.

Linux and Mac users:

> mongod --replSet m101 --logpath "1.log" --dbpath /data/rs1 --port 27017 --smallfiles --oplogSize 64 --fork
> mongod --replSet m101 --logpath "2.log" --dbpath /data/rs2 --port 27018 --smallfiles --oplogSize 64 --fork
> mongod --replSet m101 --logpath "3.log" --dbpath /data/rs3 --port 27019 --smallfiles --oplogSize 64 --fork

Windows users:

> start mongod --replSet m101 --logpath 1.log --dbpath \data\rs1 --port 27017 --smallfiles --oplogSize 64
> start mongod --replSet m101 --logpath 2.log --dbpath \data\rs2 --port 27018 --smallfiles --oplogSize 64
> start mongod --replSet m101 --logpath 3.log --dbpath \data\rs3 --port 27019 --smallfiles --oplogSize 64

Now connect to a mongo shell and make sure it comes up.

> mongo --port 27017

Now you will create the replica set. Type the following commands into the mongo shell:

> config = { _id: "m101", members:[
          { _id : 0, host : "localhost:27017"},
          { _id : 1, host : "localhost:27018"},
          { _id : 2, host : "localhost:27019"} ]};
> rs.initiate(config);

At this point, the replica set should be coming up. You can type the command below to see the state of replication.

> rs.status();

########################################
########## FINAL EXAM

https://s3.amazonaws.com/edu-downloads.10gen.com/enron/enron.zip

#### QUESTION 1

Please download the Enron email dataset (link here), unzip it and then restore it using mongorestore. Note that this is an abbreviated version of the full corpus. There should be 120,477 documents after restore.

Construct a query to calculate the number of messages sent by Andrew Fastow, CFO, to Jeff Skilling, the president. Andrew Fastow's email addess was andrew.fastow@enron.com. Jeff Skilling's email was jeff.skilling@enron.com.

For reference, the number of email messages from Andrew Fastow to John Lavorato (john.lavorato@enron.com) was 1.

> mongorestore -d enron -c messages messages.bson
> use enron
> db.messages.count() // It must return 120477 documents
> db.messages.findOne() // Checking document structure

{
        "_id" : ObjectId("4f16fc97d1e2d32371003f04"),
        "body" : "\nAUSTIN AT THE CAPITOL\n701 EAST 11TH STREET\nAUSTIN, TX  78701\ntel: 512-478-1111\nfax: 512-478-3700\n\nHotel Infor
mation: http://marriotthotels.com/AUSDT\n\n\nARRIVAL CONFIRMATION:\n Confirmation Number:83929275\nGuests in Room: 3\nNAME: MR ERIC  BA
SS \nGuest Phone: 7138530977\nNumber of Rooms:1\nArrive: Sep 7 2001\nDepart: Sep 8 2001\nRoom Type: ROOM - QUALITY\nGuarantee Method:\n
 Credit card guarantee\n\n\nRATE INFORMATION:\nRate(s) Quoted in: US DOLLAR\nArrival Date: Sep 7 2001\nRoom Rate: 129.00  per night. Pl
us tax when applicable\nRate Program: LEISURE RATE\n\nSPECIAL REQUEST:\n NON-SMOKING ROOM, GUARANTEED\n 2 DOUBLE BEDS, GUARANTEED\n HIG
H FLOOR, REQUEST NOTED\n \n\n\nPLEASE DO NOT REPLY TO THIS EMAIL \nAny Inquiries Please call 1-800-228-9290 or your local\ninternationa
l toll free number.\n \nConfirmation Sent: Mon Jul 30 15:45:05 2001\n\nLegal Disclaimer:\nThis confirmation notice has been transmitted
 to you by electronic\nmail for your convenience. Marriott's record of this confirmation\nnotice is the official record of this reserva
tion. Subsequent\nalterations to this electronic message after its transmission\nwill be disregarded.\n\nMarriott is pleased to announc
e that High Speed Internet Access is\nbeing rolled out in all Marriott hotel brands around the world.\nTo learn more or to find out whe
ther your hotel has the service\navailable, please visit Marriott.com.\n\nEarn points toward free vacations, or frequent flyer miles\nf
or every stay you make!  Just provide your Marriott Rewards\nmembership number at check in.  Not yet a member?  Join for free at\nhttps
://member.marriottrewards.com/Enrollments/enroll.asp?source=MCRE\n\n",
        "filename" : "3.",
        "headers" : {
                "Content-Transfer-Encoding" : "7bit",
                "Content-Type" : "text/plain; charset=us-ascii",
                "Date" : ISODate("2001-07-30T19:45:06Z"),
                "From" : "reservations@marriott.com",
                "Message-ID" : "<9573108.1075840323920.JavaMail.evans@thyme>",
                "Mime-Version" : "1.0",
                "Subject" : "83929275 Marriott  Reservation Confirmation Number",
                "To" : [
                        "ebass@enron.com"
                ],
                "X-FileName" : "eric bass 6-25-02.PST",
                "X-Folder" : "\\ExMerge - Bass, Eric\\Personal",
                "X-From" : "Reservations@Marriott.com",
                "X-Origin" : "BASS-E",
                "X-To" : "EBASS@ENRON.COM",
                "X-bcc" : "",
                "X-cc" : ""
        },
        "mailbox" : "bass-e",
        "subFolder" : "personal"
}

// Checking tip

> db.messages.aggregate([ 
	{ $match : { "headers.From" : { "$eq" : "andrew.fastow@enron.com" }}}, 
	{ $unwind : "$headers.To" }, 
	{ $match : { "headers.To" : { "$eq" : "john.lavorato@enron.com" }}},
	{ $group : { "_id" : "$headers.From", num_messages : { "$sum" : 1 }}}]);	
	
// Getting the answer

> db.messages.aggregate([ 
	{ $match : { "headers.From" : { "$eq" : "andrew.fastow@enron.com" }}}, 
	{ $unwind : "$headers.To" }, 
	{ $match : { "headers.To" : { "$eq" : "jeff.skilling@enron.com" }}},
	{ $group : { "_id" : "$headers.From", num_messages : { "$sum" : 1 }}}]);
	
A. 3

#### QUESTION 2

For this question you will use the aggregation framework to figure out pairs of people that tend to communicate a lot. To do this, you will need to unwind the To list for each message.

This problem is a little tricky because a recipient may appear more than once in the To list for a message. You will need to fix that in a stage of the aggregation before doing your grouping and counting of (sender, recipient) pairs.

Which pair of people have the greatest number of messages in the dataset?

1. susan.mara@enron.com to jeff.dasovich@enron.com
2. susan.mara@enron.com to richard.shapiro@enron.com
3. soblander@carrfut.com to soblander@carrfut.com
4. susan.mara@enron.com to james.steffes@enron.com
5. evelyn.metoyer@enron.com to kate.symes@enron.com
6. susan.mara@enron.com to alan.comnes@enron.com

// Checking each given statement given by the question
// As long as we must check only the message, I believe that is no need to use addToSet or something else to clean up headers.To duplicated entries

> db.messages.aggregate([ 
	{ $match : { "headers.From" : { "$eq" : "susan.mara@enron.com" }}},
	{ $match : { "headers.To" : { $elemMatch: { "$eq" : "jeff.dasovich@enron.com" } }}},
	{ $group : { "_id" : "$headers.From" , num_messages : { "$sum" : 1 }}}]); // 750 messages
	
> db.messages.aggregate([ 
	{ $match : { "headers.From" : { "$eq" : "susan.mara@enron.com" }}},
	{ $match : { "headers.To" : { $elemMatch: { "$eq" : "richard.shapiro@enron.com" } }}},
	{ $group : { "_id" : "$headers.From" , num_messages : { "$sum" : 1 }}}]); // 616 messages

> db.messages.aggregate([ 
	{ $match : { "headers.From" : { "$eq" : "soblander@carrfut.com" }}},
	{ $match : { "headers.To" : { $elemMatch: { "$eq" : "soblander@carrfut.com" } }}},
	{ $group : { "_id" : "$headers.From" , num_messages : { "$sum" : 1 }}}]); // 679 messages	

> db.messages.aggregate([ 
	{ $match : { "headers.From" : { "$eq" : "susan.mara@enron.com" }}},
	{ $match : { "headers.To" : { $elemMatch: { "$eq" : "james.steffes@enron.com" } }}},
	{ $group : { "_id" : "$headers.From" , num_messages : { "$sum" : 1 }}}]); // 646 messages

> db.messages.aggregate([ 
	{ $match : { "headers.From" : { "$eq" : "evelyn.metoyer@enron.com" }}},
	{ $match : { "headers.To" : { $elemMatch: { "$eq" : "kate.symes@enron.com" } }}},
	{ $group : { "_id" : "$headers.From" , num_messages : { "$sum" : 1 }}}]); // 567 messages	

> db.messages.aggregate([ 
	{ $match : { "headers.From" : { "$eq" : "susan.mara@enron.com" }}},
	{ $match : { "headers.To" : { $elemMatch: { "$eq" : "alan.comnes@enron.com" } }}},
	{ $group : { "_id" : "$headers.From" , num_messages : { "$sum" : 1 }}}]); // 550 messages	

A. susan.mara@enron.com to jeff.dasovich@enron.com with 750 messages
	
#### QUESTION 3

Please add the email address "mrpotatohead@mongodb.com" to the list of addresses in the "headers.To" array for the document with "headers.Message-ID" of "<8147308.1075851042335.JavaMail.evans@thyme>"

After you have completed that task, please download final3-validate-mongo-shell.js from the Download Handout link and run the following:

mongo final3-validate-mongo-shell.js

Get the validation code and use it to answer this question.

// First let's find out the document. Body field won't be included because it's huge...

> db.messages.find(
	{ "headers.Message-ID" : { $eq : "<8147308.1075851042335.JavaMail.evans@thyme>" }},
	{ body : 0 }).pretty();

{
        "_id" : ObjectId("4f16fd52d1e2d32371039e44"),
        "filename" : "1646.",
        "headers" : {
                "Content-Transfer-Encoding" : "7bit",
                "Content-Type" : "text/plain; charset=us-ascii",
                "Date" : ISODate("2001-07-13T14:47:00Z"),
                "From" : "donna.fulton@enron.com",
                "Message-ID" : "<8147308.1075851042335.JavaMail.evans@thyme>",
                "Mime-Version" : "1.0",
                "Subject" : "RTO Orders - Grid South, SE Trans, SPP and Entergy",
                "To" : [
                        "steven.kean@enron.com",
                        "richard.shapiro@enron.com",
                        "james.steffes@enron.com",
                        "christi.nicolay@enron.com",
                        "sarah.novosel@enron.com",
                        "ray.alvarez@enron.com",
                        "sscott3@enron.com",
                        "joe.connor@enron.com",
                        "dan.staines@enron.com",
                        "steve.montovano@enron.com",
                        "kevin.presto@enron.com",
                        "rogers.herndon@enron.com",
                        "mike.carson@enron.com",
                        "john.forney@enron.com",
                        "laura.podurgiel@enron.com",
                        "gretchen.lotz@enron.com",
                        "juan.hernandez@enron.com",
                        "miguel.garcia@enron.com",
                        "rudy.acevedo@enron.com",
                        "heather.kroll@enron.com",
                        "david.fairley@enron.com",
                        "elizabeth.johnston@enron.com",
                        "bill.rust@enron.com",
                        "edward.baughman@enron.com",
                        "terri.clynes@enron.com",
                        "oscar.dalton@enron.com",
                        "doug.sewell@enron.com",
                        "larry.valderrama@enron.com",
                        "nick.politis@enron.com",
                        "fletcher.sturm@enron.com",
                        "chris.dorland@enron.com",
                        "jeff.king@enron.com",
                        "john.kinser@enron.com",
                        "matt.lorenz@enron.com",
                        "patrick.hansen@enron.com",
                        "lloyd.will@enron.com",
                        "dduaran@enron.com",
                        "john.lavorato@enron.com",
                        "louise.kitchen@enron.com",
                        "greg.whalley@enron.com"
                ],
                "X-FileName" : "skean.nsf",
                "X-Folder" : "\\Steven_Kean_Oct2001_2\\Notes Folders\\Attachments",
                "X-From" : "Donna Fulton",
                "X-Origin" : "KEAN-S",
                "X-To" : "Steven J Kean, Richard Shapiro, James D Steffes, Christi L Nicolay, Sarah Novosel, Ray Alvarez, sscott3@enron
.com, Joe Connor, Dan Staines, Steve Montovano, Kevin M Presto, Rogers Herndon, Mike Carson, John M Forney, Laura Podurgiel, Gretchen L
otz, Juan Hernandez, Miguel L Garcia, Rudy Acevedo, Heather Kroll, David Fairley, Elizabeth Johnston, Bill Rust, Edward D Baughman, Ter
ri Clynes, Oscar Dalton, Doug Sewell, Larry Valderrama, Nick Politis, Fletcher J Sturm, Chris Dorland, Jeff King, John Kinser, Matt Lor
enz, Patrick Hansen, Lloyd Will, dduaran@enron.com, John J Lavorato, Louise Kitchen, Greg Whalley",
                "X-bcc" : "",
                "X-cc" : ""
        },
        "mailbox" : "kean-s",
        "subFolder" : "attachments"
}

// Now we know that we're going to update only one document, so...

> db.messages.updateOne(
	{ "headers.Message-ID" : { $eq : "<8147308.1075851042335.JavaMail.evans@thyme>" }},	
	{ $push: { "headers.To" : "mrpotatohead@mongodb.com" }}
);

> mongo final3-validate-mongo-shell_56c3a13ad8ca391cef3abe58.98637d16fdd8.js

Welcome to the Final Exam Q3 Checker. My job is to make sure you correctly updated the document
Final Exam Q3 Validated successfully!
Your validation code is: vOnRg05kwcqyEFSve96R

A. vOnRg05kwcqyEFSve96R

#### QUESTION 4

In this problem, you will be enhancing the blog project to support users liking certain comments and the like counts showing up the in the permalink page.

Start by downloading the code in final-problem4.zip and posts.json files from the Download Handout link. Load up the blog dataset posts.json. The user interface has already been implemented for you. It's not fancy. The /post URL shows the like counts next to each comment and displays a Like button that you can click on. That Like button POSTS to the /like URL on the blog, makes the necessary changes to the database state (you are implementing this), and then redirects the browser back to the permalink page.

This full round trip and redisplay of the entire web page is not how you would implement liking in a modern web app, but it makes it easier for us to reason about, so we will go with it.

Your job is to search the code for the string "XXX work here" and make the necessary changes. You can choose whatever schema you want, but you should note that the entry_template makes some assumptions about the how the like value will be encoded and if you go with a different convention than it assumes, you will need to make some adjustments.

// Importing the base...

> mongoimport -d blog -c posts < posts__f4_m101j_m101js_m101p_52d94c86e2d423744501ce90.f52bca51f2fb-part0.json
> mongoimport -d blog -c posts < posts__f4_m101j_m101js_m101p_52d94c86e2d423744501ce90.f52bca51f2fb-part1.json
> mongoimport -d blog -c posts < posts__f4_m101j_m101js_m101p_52d94c86e2d423744501ce90.f52bca51f2fb-part2.json

// Running the application...

> mvn compile exec:java -Dexec.mainClass=course.BlogController

// There are some comments with like in http://localhost:8082/post/mxwnnnqaflufnqwlekfd, so let's check the document...

> db.posts.findOne({ "permalink" : "mxwnnnqaflufnqwlekfd" },{ body: 0, comments: { $slice: 5 } })

{
        "_id" : ObjectId("5143ddf5bcf1bf4ab37da054"),
        "author" : "machine",
        "comments" : [
                {
                        "author" : "Tonia Surace",
                        "body" : "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore
et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Dui
s aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat n
on proident, sunt in culpa qui officia deserunt mollit anim id est laborum",
                        "email" : "DcwPJgyw@mCBzxAfy.com",
                        "num_likes" : 1
                },
                {
                        "author" : "Jonie Raby",
                        "body" : "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore
et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Dui
s aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat n
on proident, sunt in culpa qui officia deserunt mollit anim id est laborum",
                        "email" : "dStCPOXu@PlRdXTsu.com",
                        "num_likes" : 1
                },
                {
                        "author" : "Whitley Fears",
                        "body" : "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore
et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Dui
s aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat n
on proident, sunt in culpa qui officia deserunt mollit anim id est laborum",
                        "email" : "LVEboEqg@DdvOMbWC.com",
                        "num_likes" : 1
                },
                {
                        "body" : "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore
et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Dui
s aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat n
on proident, sunt in culpa qui officia deserunt mollit anim id est laborum",
                        "email" : "bHSctJpB@NbOMEPKQ.com",
                        "author" : "Denisha Cast"
                },
                {
                        "body" : "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore
et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Dui
s aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat n
on proident, sunt in culpa qui officia deserunt mollit anim id est laborum",
                        "email" : "xqfZUaYb@EzNCyscA.com",
                        "author" : "Gwyneth Garling"
                }
        ],
        "date" : ISODate("2013-03-16T02:50:29.595Z"),
        "permalink" : "mxwnnnqaflufnqwlekfd",
        "tags" : [
                "crab",
                "dinosaur",
                "tune",
                "deodorant",
                "cello",
                "rectangle",
                "sink",
                "quarter",
                "deodorant",
                "grape"
        ],
        "title" : "Gettysburg Address"
}

// "num_likes" is used to store the count. Sample updateOne query that I must implement in Java:

db.posts.updateOne({ "permalink" : "mxwnnnqaflufnqwlekfd" }, { $inc: { "comments.0.num_likes": 1 }});

A. I used mongoProc in order to answer this question.

#### QUESTION 5

Suppose your have a collection stuff which has the _id index:

{
  "v" : 1,
  "key" : {
    "_id" : 1
  },
  "ns" : "test.stuff",
  "name" : "_id_"
}

And one or more of the following indexes as well:

{
  "v" : 1,
  "key" : {
    "a" : 1,
    "b" : 1
  },
  "ns" : "test.stuff",
  "name" : "a_1_b_1"
}

{
  "v" : 1,
  "key" : {
    "a" : 1,
    "c" : 1
  },
  "ns" : "test.stuff",
  "name" : "a_1_c_1"
}

{
  "v" : 1,
  "key" : {
    "c" : 1
  },
  "ns" : "test.stuff",
  "name" : "c_1"
}

{
  "v" : 1,
  "key" : {
    "a" : 1,
    "b" : 1,
    "c" : -1
  },
  "ns" : "test.stuff",
  "name" : "a_1_b_1_c_-1"
}

Now suppose you want to run the following query against the collection.

> db.stuff.find({'a':{'$lt':10000}, 'b':{'$gt': 5000}}, {'a':1, 'c':1}).sort({'c':-1})

Which of the indexes could be used by MongoDB to assist in answering the query? Check all that apply.

1. _id_
2. a_1_b_1
3. a_1_c_1
4. c_1
5. a_1_b_1_c_-1

// Testing a sample scenario...

> db.q5.insertMany([{ "_id" : 0, "a" : 0, "b" : 0, "c" : 21 },
{ "_id" : 1, "a" : 0, "b" : 0, "c" : 54 },
{ "_id" : 2, "a" : 0, "b" : 1, "c" : 52 },
{ "_id" : 3, "a" : 0, "b" : 1, "c" : 17 },
{ "_id" : 4, "a" : 1, "b" : 0, "c" : 22 },
{ "_id" : 5, "a" : 1, "b" : 0, "c" : 5 },
{ "_id" : 6, "a" : 1, "b" : 1, "c" : 87 },
{ "_id" : 7, "a" : 1, "b" : 1, "c" : 97 }])

> db.q5.createIndex({ a : 1, b : 1 });
> db.q5.createIndex({ a : 1, c : 1 });
> db.q5.createIndex({ c : 1 });
> db.q5.createIndex({ a : 1, b : 1, c : -1 });

> db.q5.getIndexes();
> db.q5.explain().find({'a':{'$lt':10000}, 'b':{'$gt': 5000}}, {'a':1, 'c':1}).sort({'c':-1})

// The question is "which of the indexes COULD BE USED", thus we must get the value from indexName field

{
        "queryPlanner" : {
                "plannerVersion" : 1,
                "namespace" : "exam.q5",
                "indexFilterSet" : false,
                "parsedQuery" : {
                        "$and" : [
                                {
                                        "a" : {
                                                "$lt" : 10000
                                        }
                                },
                                {
                                        "b" : {
                                                "$gt" : 5000
                                        }
                                }
                        ]
                },
                "winningPlan" : {
                        "stage" : "PROJECTION",
                        "transformBy" : {
                                "a" : 1,
                                "c" : 1
                        },
                        "inputStage" : {
                                "stage" : "SORT",
                                "sortPattern" : {
                                        "c" : -1
                                },
                                "inputStage" : {
                                        "stage" : "SORT_KEY_GENERATOR",
                                        "inputStage" : {
                                                "stage" : "FETCH",
                                                "inputStage" : {
                                                        "stage" : "IXSCAN",
                                                        "keyPattern" : {
                                                                "a" : 1,
                                                                "b" : 1
                                                        },
                                                        "indexName" : "a_1_b_1",
                                                        "isMultiKey" : false,
                                                        "isUnique" : false,
                                                        "isSparse" : false,
                                                        "isPartial" : false,
                                                        "indexVersion" : 1,
                                                        "direction" : "forward",
                                                        "indexBounds" : {
                                                                "a" : [
                                                                        "[-1.#INF, 10000.0)"
                                                                ],
                                                                "b" : [
                                                                        "(5000.0, 1.#INF]"
                                                                ]
                                                        }
                                                }
                                        }
                                }
                        }
                },
                "rejectedPlans" : [
                        {
                                "stage" : "PROJECTION",
                                "transformBy" : {
                                        "a" : 1,
                                        "c" : 1
                                },
                                "inputStage" : {
                                        "stage" : "SORT",
                                        "sortPattern" : {
                                                "c" : -1
                                        },
                                        "inputStage" : {
                                                "stage" : "SORT_KEY_GENERATOR",
                                                "inputStage" : {
                                                        "stage" : "FETCH",
                                                        "filter" : {
                                                                "b" : {
                                                                        "$gt" : 5000
                                                                }
                                                        },
                                                        "inputStage" : {
                                                                "stage" : "IXSCAN",
                                                                "keyPattern" : {
                                                                        "a" : 1,
                                                                        "c" : 1
                                                                },
                                                                "indexName" : "a_1_c_1",
                                                                "isMultiKey" : false,
                                                                "isUnique" : false,
                                                                "isSparse" : false,
                                                                "isPartial" : false,
                                                                "indexVersion" : 1,
                                                                "direction" : "forward",
                                                                "indexBounds" : {
                                                                        "a" : [
                                                                                "[-1.#INF, 10000.0)"
                                                                        ],
                                                                        "c" : [
                                                                                "[MinKey, MaxKey]"
                                                                        ]
                                                                }
                                                        }
                                                }
                                        }
                                }
                        },
                        {
                                "stage" : "PROJECTION",
                                "transformBy" : {
                                        "a" : 1,
                                        "c" : 1
                                },
                                "inputStage" : {
                                        "stage" : "SORT",
                                        "sortPattern" : {
                                                "c" : -1
                                        },
                                        "inputStage" : {
                                                "stage" : "SORT_KEY_GENERATOR",
                                                "inputStage" : {
                                                        "stage" : "FETCH",
                                                        "inputStage" : {
                                                                "stage" : "IXSCAN",
                                                                "keyPattern" : {
                                                                        "a" : 1,
                                                                        "b" : 1,
                                                                        "c" : -1
                                                                },
                                                                "indexName" : "a_1_b_1_c_-1",
                                                                "isMultiKey" : false,
                                                                "isUnique" : false,
                                                                "isSparse" : false,
                                                                "isPartial" : false,
                                                                "indexVersion" : 1,
                                                                "direction" : "forward",
                                                                "indexBounds" : {
                                                                        "a" : [
                                                                                "[-1.#INF, 10000.0)"
                                                                        ],
                                                                        "b" : [
                                                                                "(5000.0, 1.#INF]"
                                                                        ],
                                                                        "c" : [
                                                                                "[MaxKey, MinKey]"
                                                                        ]
                                                                }
                                                        }
                                                }
                                        }
                                }
                        },
                        {
                                "stage" : "PROJECTION",
                                "transformBy" : {
                                        "a" : 1,
                                        "c" : 1
                                },
                                "inputStage" : {
                                        "stage" : "FETCH",
                                        "filter" : {
                                                "$and" : [
                                                        {
                                                                "a" : {
                                                                        "$lt" : 10000
                                                                }
                                                        },
                                                        {
                                                                "b" : {
                                                                        "$gt" : 5000
                                                                }
                                                        }
                                                ]
                                        },
                                        "inputStage" : {
                                                "stage" : "IXSCAN",
                                                "keyPattern" : {
                                                        "c" : 1
                                                },
                                                "indexName" : "c_1",
                                                "isMultiKey" : false,
                                                "isUnique" : false,
                                                "isSparse" : false,
                                                "isPartial" : false,
                                                "indexVersion" : 1,
                                                "direction" : "backward",
                                                "indexBounds" : {
                                                        "c" : [
                                                                "[MaxKey, MinKey]"
                                                        ]
                                                }
                                        }
                                }
                        }
                ]
        },
        "serverInfo" : {
                "host" : "Antunes-SRV",
                "port" : 27017,
                "version" : "3.2.6",
                "gitVersion" : "05552b562c7a0b3143a729aaa0838e558dc49b25"
        },
        "ok" : 1
}

A. a_1_b_1
A. a_1_c_1
A. c_1
A. a_1_b_1_c_-1

#### QUESTION 6

Suppose you have a collection of students of the following form:

{
    "_id" : ObjectId("50c598f582094fb5f92efb96"),
    "first_name" : "John",
    "last_name" : "Doe",
    "date_of_admission" : ISODate("2010-02-21T05:00:00Z"),
    "residence_hall" : "Fairweather",
    "has_car" : true,
    "student_id" : "2348023902",
    "current_classes" : [
        "His343",
        "Math234",
        "Phy123",
        "Art232"
    ]
}

Now suppose that basic inserts into the collection, which only include the last name, first name and student_id, are too slow (we can't do enough of them per second from our program). And while there are many potential application/hardware solutions such as batching, increasing bandwidth (or RAM), etc., which of the following listed options could potentially improve the speed of inserts?

1. Add an index on last_name, first_name if one does not already exist.
2. Remove all indexes from the collection, leaving only the index on _id in place
3. Provide a hint to MongoDB that it should not use an index for the inserts
4. Set w=0, j=false on writes
5. Build a replica set and insert data into the secondary nodes to free up the primary nodes.

A. Remove all indexes from the collection, leaving only the index on _id in place
A. Set w=0, j=false on writes

#### QUESTION 7

You have been tasked to cleanup a photo-sharing database. The database consists of two collections, albums, and images. Every image is supposed to be in an album, but there are orphan images that appear in no album.

Here are some example documents (not from the collections you will be downloading).

> db.albums.findOne()
{
    "_id" : 67
    "images" : [
        4745,
        7651,
        15247,
        17517,
        17853,
        20529,
        22640,
        27299,
        27997,
        32930,
        35591,
        48969,
        52901,
        57320,
        96342,
        99705
    ]
}

> db.images.findOne()
{ "_id" : 99705, "height" : 480, "width" : 640, "tags" : [ "dogs", "kittens", "work" ] }

From the above, you can conclude that the image with _id = 99705 is in album 67. It is not an orphan.

Your task is to write a program to remove every image from the images collection that appears in no album. Or put another way, if an image does not appear in at least one album, it's an orphan and should be removed from the images collection.

Download final7.zip from Download Handout link and use mongoimport to import the collections in albums.json and images.json.

When you are done removing the orphan images from the collection, there should be 89,737 documents in the images collection.

Hint: you might consider creating an index or two or your program will take a long time to run. As as a sanity check, there are 49,887 images that are tagged 'sunrises' before you remove the images.

What are the total number of images with the tag "sunrises" after the removal of orphans?

> mongoimport -d exam -c albums < albums.json // 1.000 documents imported
> mongoimport -d exam -c images < images.json // 100.000 documents imported

> db.images.findOne()

{
        "_id" : 1,
        "height" : 480,
        "width" : 640,
        "tags" : [
                "cats",
                "sunrises",
                "kittens",
                "travel",
                "vacation",
                "work"
        ]
}

> db.albums.findOne()

{
        "_id" : 0,
        "images" : [
                2433,
                2753,
                2983,
                6510,
                11375,
                12974,
                15344,
                16952,
                19722,
                23077,
                24772,
                31401,
                32579,
                32939,
                33434,
                36328,
                39247,
                39892,
                40597,
                45675,
                46147,
                46225,
                48406,
                49947,
                55361,
                57420,
                60101,
                62423,
                64640,
                65000,
                67203,
                68064,
                75918,
                80196,
                80642,
                82848,
                83837,
                84460,
                86419,
                87089,
                88595,
                88904,
                89308,
                91989,
                92411,
                98135,
                98548,
                99334
        ]
}


> db.albums.find({ "images" : { $elemMatch : { "$eq" : 64640 }}}) // Simple test case
> db.images.find({ "tags" : { $elemMatch : { "$eq" : 'sunrises' }}}).count() // 49.887

> db.images.createIndex({ tags: 1 });
> db.albums.createIndex({ images : 1 });
> db.images.getIndexes();
> db.albums.getIndexes();
	
> var myImagesCursor = db.images.find();
> myImagesCursor.forEach(function (myDoc){ 
	if (db.albums.find({ "images" : { $elemMatch : { "$eq" : myDoc._id }}}).count() < 1)
		db.images.remove({ "_id" : myDoc._id });
});

> db.images.find({ "tags" : { $elemMatch : { "$eq" : 'sunrises' }}}).count() // 44.787

A. 44.787

#### QUESTION 8

Supposed we executed the following Java code. How many animals will be inserted into the "animals" collection?

import com.mongodb.MongoClient;
import com.mongodb.client.MongoCollection;
import com.mongodb.client.MongoDatabase;
import org.bson.Document;

public class InsertTest {
        public static void main(String[] args) {
            MongoClient c =  new MongoClient();
            MongoDatabase db = c.getDatabase("test");
            MongoCollection<Document> animals = db.getCollection("animals");

			Document animal = new Document("animal", "monkey");

            animals.insertOne(animal);
            animal.remove("animal");
            animal.append("animal", "cat");
            animals.insertOne(animal);
            animal.remove("animal");
            animal.append("animal", "lion");
            animals.insertOne(animal);
        }
}

A. 1 because _id stays the same, resulting in the following message: E11000 duplicate key error

#### QUESTION 9

Imagine an electronic medical record database designed to hold the medical records of every individual in the United States. Because each person has more than 16MB of medical history and records, it's not feasible to have a single document for every patient. Instead, there is a patient collection that contains basic information on each person and maps the person to a patient_id, and a record collection that contains one document for each test or procedure. One patient may have dozens or even hundreds of documents in the record collection.

We need to decide on a shard key to shard the record collection. What's the best shard key for the record collection, provided that we are willing to run inefficient scatter-gather operations to do infrequent research and run studies on various diseases and cohorts? That is, think mostly about the operational aspects of such a system. And by operational, we mean, think about what the most common operations that this systems needs to perform day in and day out.

// Patient collection basic information on each person -> patient_id
// Record collection that contains one document for each test or procedure -> PATIENT collection MAY HAVE VARIOUS documents in RECORD collection

1. patient_id
2. _id
3. Primary care physician (your principal doctor that handles everyday problems)
4. Date and time when medical record was created
5. Patient first name
6. Patient last name

A. patient_id as it distributes a lot better than the other options across the shards.

#### QUESTION 10 (last one)

--> Understanding the output of explain

We perform the following query on the enron dataset:

var exp = db.messages.explain('executionStats')
exp.find( { 'headers.Date' : { '$gt' : new Date(2001,3,1) } }, { 'headers.From' : 1, '_id' : 0 } ).sort( { 'headers.From' : 1 } )

And get the following explain output.

{
  "queryPlanner" : {
    "plannerVersion" : 1,
    "namespace" : "enron.messages",
    "indexFilterSet" : false,
    "parsedQuery" : {
      "headers.Date" : {
        "$gt" : ISODate("2001-04-01T05:00:00Z")
      }
    },
    "winningPlan" : {
      "stage" : "PROJECTION",
      "transformBy" : {
        "headers.From" : 1,
        "_id" : 0
      },
      "inputStage" : {
        "stage" : "FETCH",
        "filter" : {
          "headers.Date" : {
            "$gt" : ISODate("2001-04-01T05:00:00Z")
          }
        },
        "inputStage" : {
          "stage" : "IXSCAN",
          "keyPattern" : {
            "headers.From" : 1
          },
          "indexName" : "headers.From_1",
          "isMultiKey" : false,
          "direction" : "forward",
          "indexBounds" : {
            "headers.From" : [
              "[MinKey, MaxKey]"
            ]
          }
        }
      }
    },
    "rejectedPlans" : [ ]
  },
  "executionStats" : {
    "executionSuccess" : true,
    "nReturned" : 83057,
    "executionTimeMillis" : 726,
    "totalKeysExamined" : 120477,
    "totalDocsExamined" : 120477,
    "executionStages" : {
      "stage" : "PROJECTION",
      "nReturned" : 83057,
      "executionTimeMillisEstimate" : 690,
      "works" : 120478,
      "advanced" : 83057,
      "needTime" : 37420,
      "needFetch" : 0,
      "saveState" : 941,
      "restoreState" : 941,
      "isEOF" : 1,
      "invalidates" : 0,
      "transformBy" : {
        "headers.From" : 1,
        "_id" : 0
      },
      "inputStage" : {
        "stage" : "FETCH",
        "filter" : {
          "headers.Date" : {
            "$gt" : ISODate("2001-04-01T05:00:00Z")
          }
        },
        "nReturned" : 83057,
        "executionTimeMillisEstimate" : 350,
        "works" : 120478,
        "advanced" : 83057,
        "needTime" : 37420,
        "needFetch" : 0,
        "saveState" : 941,
        "restoreState" : 941,
        "isEOF" : 1,
        "invalidates" : 0,
        "docsExamined" : 120477,
        "alreadyHasObj" : 0,
        "inputStage" : {
          "stage" : "IXSCAN",
          "nReturned" : 120477,
          "executionTimeMillisEstimate" : 60,
          "works" : 120477,
          "advanced" : 120477,
          "needTime" : 0,
          "needFetch" : 0,
          "saveState" : 941,
          "restoreState" : 941,
          "isEOF" : 1,
          "invalidates" : 0,
          "keyPattern" : {
            "headers.From" : 1
          },
          "indexName" : "headers.From_1",
          "isMultiKey" : false,
          "direction" : "forward",
          "indexBounds" : {
            "headers.From" : [
              "[MinKey, MaxKey]"
            ]
          },
          "keysExamined" : 120477,
          "dupsTested" : 0,
          "dupsDropped" : 0,
          "seenInvalidated" : 0,
          "matchTested" : 0
        }
      }
    }
  },
  "serverInfo" : {
    "host" : "dpercy-mac-air.local",
    "port" : 27017,
    "version" : "3.0.1",
    "gitVersion" : "534b5a3f9d10f00cd27737fbcd951032248b5952"
  },
  "ok" : 1
}

Check below all the statements that are true about the way MongoDB handled this query.

1. The query used an index to figure out which documents match the find criteria.
2. The query avoided sorting the documents because it was able to use an index's ordering.
3. The query returned 120,477 documents.
4. The query scanned every document in the collection.

A. The query avoided sorting the documents because it was able to use an index's ordering.
A. The query scanned every document in the collection.